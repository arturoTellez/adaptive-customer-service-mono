{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9a178caab40f4f1c9416173516deaba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32774bc8405d4841a54da066d82c3f11",
              "IPY_MODEL_ffc1dfea26e348bea71564e965984f55",
              "IPY_MODEL_62aff76d589e43e696fc31e005c19df0"
            ],
            "layout": "IPY_MODEL_880353b693ff475fa6e61779abd517e9"
          }
        },
        "32774bc8405d4841a54da066d82c3f11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3af7f4b0c7f04d04b87c32f71ca684df",
            "placeholder": "​",
            "style": "IPY_MODEL_0defcac2782542639dc29021d63af3b5",
            "value": "Generando conversaciones: 100%"
          }
        },
        "ffc1dfea26e348bea71564e965984f55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17f45adfbc8c4477a298edb4be7540a5",
            "max": 102,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3fe2023215894a2b9a1bccf5a44ca853",
            "value": 102
          }
        },
        "62aff76d589e43e696fc31e005c19df0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbcbe28684034667874deb3356a7491e",
            "placeholder": "​",
            "style": "IPY_MODEL_c4b998841363484cbe942e16d0daeffd",
            "value": " 102/102 [15:18&lt;00:00, 19.64s/conv]"
          }
        },
        "880353b693ff475fa6e61779abd517e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af7f4b0c7f04d04b87c32f71ca684df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0defcac2782542639dc29021d63af3b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17f45adfbc8c4477a298edb4be7540a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe2023215894a2b9a1bccf5a44ca853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbcbe28684034667874deb3356a7491e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4b998841363484cbe942e16d0daeffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9dadf8d1d8f49e4a28758346e36266a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa3e6ec3e26a472aa8f786c592f4f54f",
              "IPY_MODEL_993c2a1eea4b42689fd139c7b6c3d143",
              "IPY_MODEL_7d8873da0123498dab0c873627edb86d"
            ],
            "layout": "IPY_MODEL_30f2837c52af4ab38817be3a8e908d96"
          }
        },
        "fa3e6ec3e26a472aa8f786c592f4f54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_378b193c5b774abba2017c47d67ff944",
            "placeholder": "​",
            "style": "IPY_MODEL_b5bb9fb4a9a94dbb822b3beba3e791ef",
            "value": "Generando propuestas: 100%"
          }
        },
        "993c2a1eea4b42689fd139c7b6c3d143": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea47bd182010402ab605a60b7f3d6187",
            "max": 18,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ccceb035d88d4dad93b856a75ea8e003",
            "value": 18
          }
        },
        "7d8873da0123498dab0c873627edb86d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9a8ae9cdace84639a77bbae64ad28bee",
            "placeholder": "​",
            "style": "IPY_MODEL_8bbbfcd52abe4ba09a22b8f68ec32973",
            "value": " 18/18 [00:20&lt;00:00,  1.55s/conv]"
          }
        },
        "30f2837c52af4ab38817be3a8e908d96": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "378b193c5b774abba2017c47d67ff944": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5bb9fb4a9a94dbb822b3beba3e791ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea47bd182010402ab605a60b7f3d6187": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ccceb035d88d4dad93b856a75ea8e003": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9a8ae9cdace84639a77bbae64ad28bee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bbbfcd52abe4ba09a22b8f68ec32973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "openai_kavak_secret = userdata.get('OPENAI_KAVAK')\n"
      ],
      "metadata": {
        "id": "MnZL23TFwifd"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "KawuXnMgweHJ"
      },
      "outputs": [],
      "source": [
        "# pip install openai python-dotenv\n",
        "import os, json, uuid, random, csv\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any\n",
        "from dotenv import load_dotenv\n",
        "from openai import OpenAI\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Config ----------\n",
        "load_dotenv()\n",
        "#API_KEY = os.getenv(\"OPENAI_KAVAK_SECRET\") or os.getenv(\"OPENAI_API_KEY\")\n",
        "#if not API_KEY:\n",
        "#    raise RuntimeError(\"Falta OPENAI_KAVAK_SECRET u OPENAI_API_KEY en tu entorno/.env\")\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5\")\n",
        "\n",
        "client = OpenAI(api_key=openai_kavak_secret)\n",
        "\n",
        "CONTEXTS = ['buying', 'ask', 'feedback', 'service', 'credit', 'warranty']\n",
        "TONOS = [\"amable\", \"empático\", \"formal\", \"resolutivo\", \"apologético\", \"directo\", \"entusiasta\"]\n",
        "CANALES = [\"whatsapp\", \"webchat\", \"email\", \"telefono\"]\n",
        "IDIOMAS = [\"es\", \"es\", \"es\", \"es\", \"en\"]  # mostly ES\n",
        "\n",
        "# ---------- Prompts ----------\n",
        "def system_prompt() -> str:\n",
        "    return (\n",
        "        \"Eres un generador de conversaciones realistas de atención a clientes para Kavak. \"\n",
        "        \"Prioriza claridad, empatía y cumplimiento. No inventes datos sensibles. \"\n",
        "        \"Mantén diálogos breves y creíbles, en el idioma indicado.\"\n",
        "    )\n",
        "\n",
        "ESCENARIOS = {\n",
        "    \"buying\":   \"Quiere vender su coche y recibir oferta <24h. Pide proceso, documentos y tiempos.\",\n",
        "    \"ask\":      \"Pregunta estado de evaluación mecánica y tiempos de pago.\",\n",
        "    \"feedback\": \"Queja por retraso en transferencia; pide compensación.\",\n",
        "    \"service\":  \"Solicita reprogramar inspección a domicilio por cambio de agenda.\",\n",
        "    \"credit\":   \"Pregunta si califica a crédito para comprar auto y posibles tasas.\",\n",
        "    \"warranty\": \"Duda si una falla eléctrica entra en garantía extendida y cómo tramitarla.\"\n",
        "}\n",
        "#ESCENARIOS = {\n",
        "#\n",
        "#}\n",
        "\n",
        "def user_prompt(contexto: str, tono: str, idioma: str, canal: str) -> str:\n",
        "    return f\"\"\"\n",
        "Crea una conversación breve entre **agente de Kavak** y **cliente**.\n",
        "- contexto: {contexto}\n",
        "- tono: {tono}\n",
        "- idioma: {\"español\" if idioma==\"es\" else \"inglés\"}\n",
        "- canal: {canal}\n",
        "\n",
        "Requisitos:\n",
        "1) Primer turno del **cliente**.\n",
        "2) Cumple políticas (documentos, inspección, pagos, garantías, crédito, KYC si aplica).\n",
        "3) Si no se resuelve, deja claro el siguiente paso (ticket, escalar, cita, docs).\n",
        "4) Respuestas concisas, naturales (no robóticas).\n",
        "5) Devuelve **solo un JSON** con claves: meta, transcript, outcomes.\n",
        "\n",
        "Escenario: {ESCENARIOS[contexto]}\n",
        "\n",
        "Estructura del JSON esperado (campos mínimos):\n",
        "{{\n",
        "  \"meta\": {{\n",
        "    \"conversation_id\": \"\", \"company\": \"Kavak\", \"context\": \"{contexto}\",\n",
        "    \"channel\": \"{canal}\", \"tone\": \"{tono}\", \"language\": \"{idioma}\",\n",
        "    \"customer_issue\": \"\", \"customer_goal\": \"\", \"agent_goal\": \"\",\n",
        "    \"resolved\": true, \"num_interactions\": 0, \"duration_sec\": 0\n",
        "  }},\n",
        "  \"transcript\": [\n",
        "    {{\"turn\": 1, \"speaker\": \"cliente\", \"text\": \"\", \"timestamp\": \"\"}},\n",
        "    {{\"turn\": 2, \"speaker\": \"agente\",  \"text\": \"\", \"timestamp\": \"\"}}\n",
        "  ],\n",
        "  \"outcomes\": {{\n",
        "    \"csat_estimated_1_5\": 3, \"next_action\": \"\", \"followup_needed\": false, \"summary\": \"\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "IMPORTANTE:\n",
        "- Responde **solo** el JSON (sin ``` ni texto extra).\n",
        "- Si falta algo, rellénalo con valores razonables.\n",
        "\"\"\".strip()\n",
        "\n",
        "# ---------- Utilidades ----------\n",
        "def ensure_defaults(data: Dict[str, Any], contexto: str, canal: str, tono: str, idioma: str) -> Dict[str, Any]:\n",
        "    # Estructuras base\n",
        "    data.setdefault(\"meta\", {})\n",
        "    data.setdefault(\"transcript\", [])\n",
        "    data.setdefault(\"outcomes\", {})\n",
        "    meta = data[\"meta\"]\n",
        "    tx = data[\"transcript\"]\n",
        "    outc = data[\"outcomes\"]\n",
        "\n",
        "    # Meta\n",
        "    meta.setdefault(\"conversation_id\", str(uuid.uuid4()))\n",
        "    meta[\"company\"] = \"Kavak\"\n",
        "    meta[\"context\"] = contexto\n",
        "    meta[\"channel\"] = meta.get(\"channel\") or canal\n",
        "    meta[\"tone\"] = meta.get(\"tone\") or tono\n",
        "    meta[\"language\"] = meta.get(\"language\") or idioma\n",
        "    meta.setdefault(\"customer_issue\", \"\")\n",
        "    meta.setdefault(\"customer_goal\", \"\")\n",
        "    meta.setdefault(\"agent_goal\", \"\")\n",
        "    meta.setdefault(\"resolved\", True)\n",
        "    meta.setdefault(\"num_interactions\", 0)\n",
        "    meta.setdefault(\"duration_sec\", 0)\n",
        "\n",
        "    # Transcript: turn + timestamps\n",
        "    t0 = datetime.utcnow()\n",
        "    if not tx:\n",
        "        tx.extend([\n",
        "            {\"turn\": 1, \"speaker\": \"cliente\", \"text\": \"Hola, ¿me pueden apoyar?\", \"timestamp\": \"\"},\n",
        "            {\"turn\": 2, \"speaker\": \"agente\", \"text\": \"Con gusto, ¿puedes compartirme el folio o placas?\", \"timestamp\": \"\"}\n",
        "        ])\n",
        "    for i, t in enumerate(tx):\n",
        "        t[\"turn\"] = i + 1\n",
        "        if \"speaker\" not in t or t[\"speaker\"] not in (\"cliente\", \"agente\"):\n",
        "            t[\"speaker\"] = \"cliente\" if i % 2 == 0 else \"agente\"\n",
        "        t[\"text\"] = t.get(\"text\") or \"\"\n",
        "        t[\"timestamp\"] = t.get(\"timestamp\") or (t0 + timedelta(seconds=5*i)).isoformat() + \"Z\"\n",
        "\n",
        "    # Duración estimada\n",
        "    meta[\"num_interactions\"] = len(tx)\n",
        "    estimated = max((len(tx) - 1) * 5, 45)\n",
        "    try:\n",
        "        given = int(meta.get(\"duration_sec\") or 0)\n",
        "    except (TypeError, ValueError):\n",
        "        given = 0\n",
        "    meta[\"duration_sec\"] = max(given, estimated)\n",
        "\n",
        "    # Outcomes\n",
        "    outc.setdefault(\"csat_estimated_1_5\", 3)\n",
        "    outc.setdefault(\"next_action\", \"\")\n",
        "    outc.setdefault(\"followup_needed\", not bool(meta.get(\"resolved\", True)))\n",
        "    outc.setdefault(\"summary\", \"\")\n",
        "\n",
        "    return data\n",
        "\n",
        "# ---------- Core ----------\n",
        "def generate_one_conversation(contexto: str, seed: int = None) -> Dict[str, Any]:\n",
        "    random.seed(seed or random.randint(1, 10_000))\n",
        "    tono = random.choice(TONOS)\n",
        "    canal = random.choice(CANALES)\n",
        "    idioma = random.choice(IDIOMAS)\n",
        "\n",
        "    uprompt = user_prompt(contexto, tono, idioma, canal)\n",
        "\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=1,\n",
        "        #top_p=0.95,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt()},\n",
        "            {\"role\": \"user\",   \"content\": uprompt}\n",
        "        ]\n",
        "    )\n",
        "    content = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "    # Quita fences si vinieran por accidente\n",
        "    if content.startswith(\"```\"):\n",
        "        content = content.strip(\"`\")\n",
        "        lines = content.splitlines()\n",
        "        if lines and lines[0].lower().startswith(\"json\"):\n",
        "            content = \"\\n\".join(lines[1:])\n",
        "\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        # Fallback ultra-simple si el modelo no obedeció\n",
        "        data = {\"meta\": {}, \"transcript\": [], \"outcomes\": {}}\n",
        "\n",
        "    return ensure_defaults(data, contexto, canal, tono, idioma)\n",
        "\n",
        "def generate_dataset(contexts: List[str], per_context: int = 2, seed: int = 123) -> List[Dict[str, Any]]:\n",
        "    random.seed(seed)\n",
        "    out = []\n",
        "    for c in contexts:\n",
        "        for _ in range(per_context):\n",
        "            out.append(generate_one_conversation(c, seed=random.randint(1, 10_000)))\n",
        "    return out\n",
        "\n",
        "def save_jsonl(rows: List[Dict[str, Any]], path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "def save_csv_meta(rows: List[Dict[str, Any]], path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    fields = [\n",
        "        \"conversation_id\",\"company\",\"context\",\"channel\",\"tone\",\"language\",\n",
        "        \"customer_issue\",\"customer_goal\",\"agent_goal\",\"resolved\",\n",
        "        \"num_interactions\",\"duration_sec\",\n",
        "        \"csat_estimated_1_5\",\"next_action\",\"followup_needed\",\"summary\"\n",
        "    ]\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fields)\n",
        "        w.writeheader()\n",
        "        for r in rows:\n",
        "            meta = r.get(\"meta\", {})\n",
        "            outc = r.get(\"outcomes\", {})\n",
        "            w.writerow({\n",
        "                \"conversation_id\": meta.get(\"conversation_id\",\"\"),\n",
        "                \"company\": meta.get(\"company\",\"Kavak\"),\n",
        "                \"context\": meta.get(\"context\",\"\"),\n",
        "                \"channel\": meta.get(\"channel\",\"\"),\n",
        "                \"tone\": meta.get(\"tone\",\"\"),\n",
        "                \"language\": meta.get(\"language\",\"\"),\n",
        "                \"customer_issue\": meta.get(\"customer_issue\",\"\"),\n",
        "                \"customer_goal\": meta.get(\"customer_goal\",\"\"),\n",
        "                \"agent_goal\": meta.get(\"agent_goal\",\"\"),\n",
        "                \"resolved\": meta.get(\"resolved\", True),\n",
        "                \"num_interactions\": meta.get(\"num_interactions\", 0),\n",
        "                \"duration_sec\": meta.get(\"duration_sec\", 0),\n",
        "                \"csat_estimated_1_5\": outc.get(\"csat_estimated_1_5\", 3),\n",
        "                \"next_action\": outc.get(\"next_action\",\"\"),\n",
        "                \"followup_needed\": outc.get(\"followup_needed\", False),\n",
        "                \"summary\": outc.get(\"summary\",\"\"),\n",
        "            })\n",
        "\n",
        "import uuid\n",
        "from datetime import datetime\n",
        "import uuid, csv, os\n",
        "from datetime import datetime\n",
        "\n",
        "# Perfil del bot (el que pegaste)\n",
        "BOT_PROFILE = {\n",
        "    \"id\": \"1fae168d-e800-460b-98ec-1f7252411a6f\",\n",
        "    \"email\": \"demo-agent@gmail.com\",\n",
        "    \"name\": \"Demo Kavak\",\n",
        "}\n",
        "\n",
        "# Namespace fijo para generar UUID5 desde ticket ids textuales\n",
        "NAMESPACE_TICKET = uuid.UUID(\"12345678-1234-5678-1234-567812345678\")\n",
        "\n",
        "def _to_pg_naive(ts: str) -> str:\n",
        "    if not ts:\n",
        "        return \"\"\n",
        "    try:\n",
        "        ts = ts.replace(\"Z\", \"\").replace(\"T\", \" \")\n",
        "        dt = datetime.fromisoformat(ts)\n",
        "        return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return \"\"\n",
        "\n",
        "def _coerce_uuid(s: str) -> str:\n",
        "    \"\"\"Devuelve UUID válido: si s ya es UUID lo respeta; si no, genera uuid5 determinístico.\"\"\"\n",
        "    if not s:\n",
        "        return str(uuid.uuid4())\n",
        "    try:\n",
        "        return str(uuid.UUID(str(s)))\n",
        "    except Exception:\n",
        "        return str(uuid.uuid5(NAMESPACE_TICKET, str(s)))\n",
        "\n",
        "def save_csv_messages(rows, path: str, bot_profile: dict = BOT_PROFILE):\n",
        "    \"\"\"\n",
        "    Exporta CSV compatible con public.messages:\n",
        "    columnas: id, ticket_id, content, is_bot, sender_name, created_at\n",
        "    - ticket_id siempre UUID (uuid5 si venía texto).\n",
        "    - is_bot True → sender_name = bot_profile['name'] (p.ej., \"Demo Kavak\")\n",
        "      is_bot False → sender_name = \"cliente\"\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    fields = [\"id\", \"ticket_id\", \"content\", \"is_bot\", \"sender_name\", \"created_at\"]\n",
        "\n",
        "    with open(path, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
        "        w = csv.DictWriter(f, fieldnames=fields)\n",
        "        w.writeheader()\n",
        "\n",
        "        for conv in rows:\n",
        "            meta = conv.get(\"meta\", {})\n",
        "            raw_ticket = meta.get(\"conversation_id\") or meta.get(\"ticket_id\") or \"\"\n",
        "            ticket_id = _coerce_uuid(raw_ticket)\n",
        "\n",
        "            for msg in conv.get(\"transcript\", []):\n",
        "                sender = (msg.get(\"speaker\") or \"\").strip().lower()\n",
        "                is_bot = (sender == \"agente\")\n",
        "\n",
        "                w.writerow({\n",
        "                    \"id\": str(uuid.uuid4()),\n",
        "                    \"ticket_id\": ticket_id,\n",
        "                    \"content\": (msg.get(\"text\") or \"\").strip(),\n",
        "                    \"is_bot\": is_bot,\n",
        "                    \"sender_name\": (bot_profile.get(\"name\") or \"agente\") if is_bot else \"cliente\",\n",
        "                    \"created_at\": _to_pg_naive(msg.get(\"timestamp\", \"\")),  # puedes omitir para usar DEFAULT\n",
        "                })\n",
        "\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text\n",
        "\n",
        "# --- Reusa tu BOT_PROFILE / NAMESPACE_TICKET / _to_pg_naive / _coerce_uuid ya definidos ---\n",
        "\n",
        "def _from_iso_or_none(ts: str):\n",
        "    \"\"\"\n",
        "    Devuelve 'YYYY-MM-DD HH:MM:SS' o None si no parsea (para usar DEFAULT en DB).\n",
        "    \"\"\"\n",
        "    s = _to_pg_naive(ts or \"\")\n",
        "    return s if s else None\n",
        "\n",
        "def _status_from_meta(meta: dict) -> str:\n",
        "    \"\"\"\n",
        "    Mapear 'resolved' boolean a estados válidos en tu CHECK:\n",
        "    - True  -> 'resolved'\n",
        "    - False -> 'open' (puedes cambiar a 'in_progress' si prefieres)\n",
        "    \"\"\"\n",
        "    return \"resolved\" if bool(meta.get(\"resolved\", True)) else \"open\"\n",
        "\n",
        "def _build_ticket_record(conv: dict, user_id: str | None = None) -> dict:\n",
        "    meta = conv.get(\"meta\", {})\n",
        "    tx = conv.get(\"transcript\", [])\n",
        "\n",
        "    # ID de ticket (UUID) a partir de conversation_id (si ya es UUID lo respeta; si era texto usa uuid5 determinístico)\n",
        "    raw_ticket = meta.get(\"conversation_id\") or meta.get(\"ticket_id\") or \"\"\n",
        "    ticket_id = _coerce_uuid(raw_ticket)\n",
        "\n",
        "    # Timestamps del ticket: 1er y último mensaje\n",
        "    created_at = _from_iso_or_none(tx[0].get(\"timestamp\")) if tx else None\n",
        "    updated_at = _from_iso_or_none(tx[-1].get(\"timestamp\")) if tx else None\n",
        "\n",
        "    # Campos del ticket\n",
        "    context = meta.get(\"context\") or \"general\"\n",
        "    title = meta.get(\"customer_issue\") or f\"[{context}] Nuevo caso\"\n",
        "    # description rica combinando customer_issue / summary / primer turno\n",
        "    first_msg = tx[0][\"text\"] if tx and tx[0].get(\"text\") else \"\"\n",
        "    summary = (conv.get(\"outcomes\", {}) or {}).get(\"summary\") or \"\"\n",
        "    description = (meta.get(\"customer_issue\") or \"\").strip()\n",
        "    if summary:\n",
        "        description = (description + \"\\n\\nResumen:\\n\" + summary).strip()\n",
        "    if not description and first_msg:\n",
        "        description = first_msg[:1000]\n",
        "\n",
        "    status = _status_from_meta(meta)\n",
        "    category = context\n",
        "\n",
        "    return {\n",
        "        \"id\": ticket_id,\n",
        "        \"user_id\": user_id,            # si no tienes user, deja None\n",
        "        \"title\": title[:255] or f\"[{context}] Caso\",\n",
        "        \"category\": category[:100],\n",
        "        \"description\": description or f\"Caso {context}\",\n",
        "        \"status\": status,              # 'open' | 'in_progress' | 'resolved' | 'closed'\n",
        "        \"created_at\": created_at,      # None => DEFAULT CURRENT_TIMESTAMP\n",
        "        \"updated_at\": updated_at,      # None => DEFAULT CURRENT_TIMESTAMP (trigger luego actualiza)\n",
        "    }\n",
        "\n",
        "def _build_message_records(conv: dict, ticket_id: str, bot_name: str) -> list[dict]:\n",
        "    rows = []\n",
        "    for msg in conv.get(\"transcript\", []):\n",
        "        sender = (msg.get(\"speaker\") or \"\").strip().lower()\n",
        "        is_bot = (sender == \"agente\")\n",
        "        rows.append({\n",
        "            \"id\": str(uuid.uuid4()),\n",
        "            \"ticket_id\": ticket_id,\n",
        "            \"content\": (msg.get(\"text\") or \"\").strip(),\n",
        "            \"is_bot\": is_bot,\n",
        "            \"sender_name\": bot_name if is_bot else \"cliente\",\n",
        "            \"created_at\": _from_iso_or_none(msg.get(\"timestamp\")),\n",
        "        })\n",
        "    return rows\n",
        "\n",
        "def insert_tickets_and_messages(rows: list[dict], database_url: str, default_user_id: str | None = None,\n",
        "                                bot_name: str = (BOT_PROFILE.get(\"name\") if 'BOT_PROFILE' in globals() else \"Demo Kavak\")):\n",
        "    \"\"\"\n",
        "    Inserta UN ticket por conversación + todos los mensajes asociados.\n",
        "    - rows: lista de conversaciones (las que ya generas en memoria)\n",
        "    - database_url: cadena SQLAlchemy 'postgresql+psycopg2://USER:PASS@HOST:PORT/DB'\n",
        "    - default_user_id: si quieres asociar el ticket a un usuario (puede ser None)\n",
        "    - bot_name: nombre que verá 'sender_name' cuando is_bot=True\n",
        "    \"\"\"\n",
        "    engine = sqlalchemy.create_engine(\n",
        "        database_url,\n",
        "        poolclass=NullPool,            # <-- sin pool\n",
        "        connect_args={\"sslmode\": \"require\", \"connect_timeout\": 10},\n",
        "    )\n",
        "    sql_ticket = text(\"\"\"\n",
        "        INSERT INTO public.tickets\n",
        "            (id, user_id, title, category, description, status, created_at, updated_at)\n",
        "        VALUES\n",
        "            (:id, :user_id, :title, :category, :description, :status,\n",
        "             COALESCE(:created_at, CURRENT_TIMESTAMP),\n",
        "             COALESCE(:updated_at, CURRENT_TIMESTAMP))\n",
        "        ON CONFLICT (id) DO NOTHING\n",
        "    \"\"\")\n",
        "\n",
        "    sql_message = text(\"\"\"\n",
        "        INSERT INTO public.messages\n",
        "            (id, ticket_id, content, is_bot, sender_name, created_at)\n",
        "        VALUES\n",
        "            (:id, :ticket_id, :content, :is_bot, :sender_name, COALESCE(:created_at, CURRENT_TIMESTAMP))\n",
        "    \"\"\")\n",
        "\n",
        "    # Construye los lotes\n",
        "    ticket_batch = []\n",
        "    message_batch = []\n",
        "    for conv in rows:\n",
        "        t = _build_ticket_record(conv, user_id=default_user_id)\n",
        "        ticket_batch.append(t)\n",
        "        message_batch.extend(_build_message_records(conv, t[\"id\"], bot_name))\n",
        "\n",
        "    # Inserta en una transacción\n",
        "    with engine.begin() as conn:\n",
        "        # Tickets (idempotente por ON CONFLICT (id) DO NOTHING)\n",
        "        if ticket_batch:\n",
        "            conn.execute(sql_ticket, ticket_batch)\n",
        "        # Mensajes\n",
        "        if message_batch:\n",
        "            conn.execute(sql_message, message_batch)\n",
        "\n",
        "    print(f\"✅ Insertados {len(ticket_batch)} tickets y {len(message_batch)} mensajes.\")\n",
        "    ngine.dispose()\n",
        "\n",
        "\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "import time\n",
        "from random import random as _rand\n",
        "try:\n",
        "    from tqdm.auto import tqdm  # barra de progreso bonita\n",
        "except Exception:\n",
        "    tqdm = None\n",
        "\n",
        "\n",
        "def _gen_with_retries(contexto: str, seed: int, max_attempts: int = 4, base_delay: float = 1.3):\n",
        "    \"\"\"\n",
        "    Llama generate_one_conversation con reintentos.\n",
        "    Backoff exponencial con jitter: (base_delay ** intento) + [0..0.25]s\n",
        "    \"\"\"\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            return generate_one_conversation(contexto, seed=seed)\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            if attempt >= max_attempts:\n",
        "                raise\n",
        "            sleep_s = (base_delay ** attempt) + (0.25 * _rand())\n",
        "            print(f\"⚠️  Error {contexto} (seed={seed}). Reintento {attempt}/{max_attempts-1} en {sleep_s:.2f}s -> {e}\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "\n",
        "def generate_dataset(contexts: List[str],\n",
        "                     per_context: int = 2,\n",
        "                     seed: int = 123,\n",
        "                     max_workers: int = 8,\n",
        "                     max_attempts: int = 4) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Genera conversaciones en paralelo con hilos (IO-bound).\n",
        "    - max_workers: nº de hilos concurrentes (ajusta a tus límites de API)\n",
        "    - max_attempts: reintentos por tarea\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    # Lista de tareas (contexto, seed_por_tarea) pre-generada para no competir por random en hilos\n",
        "    tasks = [(c, random.randint(1, 10_000)) for c in contexts for _ in range(per_context)]\n",
        "    results: List[Dict[str, Any]] = []\n",
        "\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        futures = {executor.submit(_gen_with_retries, c, s, max_attempts): (c, s) for (c, s) in tasks}\n",
        "\n",
        "        if tqdm:\n",
        "            iterator = tqdm(as_completed(futures), total=len(tasks), desc=\"Generando conversaciones\", unit=\"conv\")\n",
        "        else:\n",
        "            iterator = as_completed(futures)\n",
        "\n",
        "        for fut in iterator:\n",
        "            c, s = futures[fut]\n",
        "            try:\n",
        "                results.append(fut.result())\n",
        "            except Exception as e:\n",
        "                # Sigue con las demás tareas aunque una falle definitivamente\n",
        "                print(f\"❌ Falló definitivamente {c} (seed={s}): {e}\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Ajusta per_context para tu volumen (ej. 17 -> ~102 convs)\n",
        "    rows = generate_dataset(\n",
        "        CONTEXTS,\n",
        "        per_context=17,\n",
        "        seed=123,\n",
        "        max_workers=8,     # baja a 4–6 si ves rate limits\n",
        "        max_attempts=4\n",
        "    )\n",
        "\n",
        "    save_jsonl(rows, \"synthetic_kavak/conversations.jsonl\")\n",
        "    save_csv_meta(rows, \"synthetic_kavak/conversations_meta.csv\")\n",
        "    save_csv_messages(rows, \"synthetic_kavak/messages.csv\")\n",
        "    print(\"OK -> synthetic_kavak/conversations.jsonl & conversations_meta.csv & messages.csv\")\n",
        "\n",
        "    # Inserción a DB con tu función actual (si la activas):\n",
        "    # insert_tickets_and_messages(rows, database_url=\"postgresql://...\", default_user_id=\"1fae168d-e800-460b-98ec-1f7252411a6f\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "9a178caab40f4f1c9416173516deaba3",
            "32774bc8405d4841a54da066d82c3f11",
            "ffc1dfea26e348bea71564e965984f55",
            "62aff76d589e43e696fc31e005c19df0",
            "880353b693ff475fa6e61779abd517e9",
            "3af7f4b0c7f04d04b87c32f71ca684df",
            "0defcac2782542639dc29021d63af3b5",
            "17f45adfbc8c4477a298edb4be7540a5",
            "3fe2023215894a2b9a1bccf5a44ca853",
            "cbcbe28684034667874deb3356a7491e",
            "c4b998841363484cbe942e16d0daeffd"
          ]
        },
        "id": "iuYZ3eOLxI_R",
        "outputId": "1c1d0838-a9c6-4218-cd9a-47cb23f20cc0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generando conversaciones:   0%|          | 0/102 [00:00<?, ?conv/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9a178caab40f4f1c9416173516deaba3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-939169781.py:108: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  t0 = datetime.utcnow()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK -> synthetic_kavak/conversations.jsonl & conversations_meta.csv & messages.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install sqlalchemy psycopg2-binary python-dotenv tqdm\n",
        "import os, csv, uuid, math\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any, Iterable\n",
        "from dotenv import load_dotenv\n",
        "import sqlalchemy\n",
        "from sqlalchemy import text\n",
        "try:\n",
        "    from tqdm.auto import tqdm\n",
        "except Exception:\n",
        "    tqdm = None\n",
        "import sqlalchemy\n",
        "from sqlalchemy.pool import NullPool\n",
        "\n",
        "# ============== Config ==============\n",
        "load_dotenv()\n",
        "DATABASE_URL = userdata.get(\"DATABASE_URL\")\n",
        "META_CSV      = os.getenv(\"META_CSV\", \"synthetic_kavak/conversations_meta.csv\")\n",
        "MESSAGES_CSV  = os.getenv(\"MESSAGES_CSV\", \"synthetic_kavak/messages.csv\")\n",
        "DEFAULT_USER_ID = os.getenv(\"DEFAULT_USER_ID\", \"1fae168d-e800-460b-98ec-1f7252411a6f\")  # opcional: UUID del usuario dueño del ticket\n",
        "CHUNK_SIZE = int(os.getenv(\"CHUNK_SIZE\", \"5000\"))  # tamaño de lote para insert masivo\n",
        "\n",
        "# Si quieres forzar deduplicación de mensajes por (ticket_id, content, created_at),\n",
        "# primero crea el índice único y pon DEDUP_MESSAGES=True\n",
        "DEDUP_MESSAGES = os.getenv(\"DEDUP_MESSAGES\", \"false\").lower() in (\"true\",\"1\",\"y\",\"yes\",\"si\",\"sí\")\n",
        "\n",
        "# Namespace determinístico para uuid5 (normalizar ticket ids textuales)\n",
        "UUID5_NAMESPACE = uuid.NAMESPACE_DNS  # puedes fijar otro: uuid.UUID(\"12345678-1234-5678-1234-567812345678\")\n",
        "\n",
        "# ============== Helpers ==============\n",
        "def _load_csv_as_dicts(path: str) -> List[Dict[str, Any]]:\n",
        "    rows: List[Dict[str, Any]] = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        r = csv.DictReader(f)\n",
        "        for row in r:\n",
        "            rows.append({k: (v if v is not None else \"\") for k, v in row.items()})\n",
        "    return rows\n",
        "\n",
        "def _coerce_uuid_str(val: str) -> str:\n",
        "    \"\"\"Devuelve un UUID. Si val ya es UUID válido, lo respeta; si no, genera uuid5 determinístico.\"\"\"\n",
        "    s = (val or \"\").strip()\n",
        "    if not s:\n",
        "        return str(uuid.uuid4())\n",
        "    try:\n",
        "        return str(uuid.UUID(s))\n",
        "    except Exception:\n",
        "        return str(uuid.uuid5(UUID5_NAMESPACE, s))\n",
        "\n",
        "def _to_ts_or_none(s: str) -> str | None:\n",
        "    \"\"\"Convierte ISO/naive a 'YYYY-MM-DD HH:MM:SS'; si no parsea, None (usará DEFAULT en DB).\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        s2 = s.replace(\"T\", \" \").replace(\"Z\", \"\")\n",
        "        dt = datetime.fromisoformat(s2)\n",
        "        return dt.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "    except Exception:\n",
        "        return None\n",
        "\n",
        "def _status_from_resolved(resolved_val) -> str:\n",
        "    \"\"\"Mapea meta.resolved -> status de tickets: resolved/open.\"\"\"\n",
        "    try:\n",
        "        return \"resolved\" if str(resolved_val).strip().lower() in (\"true\",\"t\",\"1\",\"yes\",\"y\",\"si\",\"sí\") else \"open\"\n",
        "    except Exception:\n",
        "        return \"open\"\n",
        "\n",
        "def _chunked(iterable: Iterable[dict], size: int) -> Iterable[list]:\n",
        "    batch = []\n",
        "    for item in iterable:\n",
        "        batch.append(item)\n",
        "        if len(batch) >= size:\n",
        "            yield batch\n",
        "            batch = []\n",
        "    if batch:\n",
        "        yield batch\n",
        "\n",
        "def build_ticket_batch(meta_rows: List[Dict[str, Any]], messages_by_ticket: Dict[str, List[Dict[str, Any]]]) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Construye el batch de tickets asegurando:\n",
        "      - 1 ticket por conversación (meta: conversation_id -> ticket_id UUID)\n",
        "      - Si existen mensajes con ticket_id sin meta, crea ticket \"fallback\"\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "\n",
        "    # --- 1) Deduplicar por conversation_id en meta ---\n",
        "    meta_by_conv: Dict[str, Dict[str, Any]] = {}\n",
        "    for m in meta_rows:\n",
        "        raw_conv_id = (m.get(\"conversation_id\") or \"\").strip()\n",
        "        if not raw_conv_id:\n",
        "            raw_conv_id = str(uuid.uuid4())\n",
        "            m[\"conversation_id\"] = raw_conv_id\n",
        "        if raw_conv_id not in meta_by_conv:\n",
        "            meta_by_conv[raw_conv_id] = m  # conserva la primera ocurrencia\n",
        "\n",
        "    # --- 2) Tickets desde meta (uno por conversación) ---\n",
        "    seen_ticket_ids: set[str] = set()\n",
        "    for conv_id, m in meta_by_conv.items():\n",
        "        ticket_id = _coerce_uuid_str(conv_id)  # determinístico por conversación\n",
        "        if ticket_id in seen_ticket_ids:\n",
        "            continue\n",
        "        seen_ticket_ids.add(ticket_id)\n",
        "\n",
        "        status = _status_from_resolved(m.get(\"resolved\", \"\"))\n",
        "        category = (m.get(\"context\") or \"general\")[:100]\n",
        "        title = (m.get(\"customer_issue\") or f\"[{category}] Nuevo caso\")[:255]\n",
        "\n",
        "        # description = customer_issue + summary, o primer mensaje cliente\n",
        "        description = (m.get(\"customer_issue\") or \"\").strip()\n",
        "        summary = (m.get(\"summary\") or \"\").strip()\n",
        "        if summary:\n",
        "            description = (description + \"\\n\\nResumen:\\n\" + summary).strip()\n",
        "        if (not description) and (ticket_id in messages_by_ticket):\n",
        "            first_client = next(\n",
        "                (mm for mm in messages_by_ticket[ticket_id]\n",
        "                 if not str(mm.get(\"is_bot\", \"\")).strip().lower() in (\"true\",\"t\",\"1\",\"yes\",\"y\",\"si\",\"sí\")),\n",
        "                None\n",
        "            )\n",
        "            if first_client:\n",
        "                description = (first_client.get(\"content\") or \"\")[:1000]\n",
        "        if not description:\n",
        "            description = f\"Caso {category}\"\n",
        "\n",
        "        # timestamps del ticket: min/max de mensajes\n",
        "        created_at = None\n",
        "        updated_at = None\n",
        "        if ticket_id in messages_by_ticket:\n",
        "            times = []\n",
        "            for mm in messages_by_ticket[ticket_id]:\n",
        "                ts = _to_ts_or_none(mm.get(\"created_at\", \"\"))\n",
        "                if ts:\n",
        "                    times.append(ts)\n",
        "            if times:\n",
        "                created_at = min(times)\n",
        "                updated_at = max(times)\n",
        "\n",
        "        batch.append({\n",
        "            \"id\": ticket_id,\n",
        "            \"user_id\": DEFAULT_USER_ID,\n",
        "            \"title\": title,\n",
        "            \"category\": category,\n",
        "            \"description\": description,\n",
        "            \"status\": status,\n",
        "            \"created_at\": created_at,\n",
        "            \"updated_at\": updated_at,\n",
        "        })\n",
        "\n",
        "    # --- 3) Tickets \"fallback\" si hay mensajes sin meta correspondiente ---\n",
        "    for msg_ticket_id, msgs in messages_by_ticket.items():\n",
        "        if msg_ticket_id in seen_ticket_ids:\n",
        "            continue  # ya creado por meta\n",
        "        seen_ticket_ids.add(msg_ticket_id)\n",
        "\n",
        "        # Primer mensaje para construir título/desc\n",
        "        first_msg = msgs[0] if msgs else {}\n",
        "        first_client = next(\n",
        "            (mm for mm in msgs\n",
        "             if not str(mm.get(\"is_bot\", \"\")).strip().lower() in (\"true\",\"t\",\"1\",\"yes\",\"y\",\"si\",\"sí\")),\n",
        "            None\n",
        "        )\n",
        "        title = \"[general] Nuevo caso\"[:255]\n",
        "        description = (first_client or first_msg).get(\"content\", \"\")[:1000] if (first_client or first_msg) else \"Caso general\"\n",
        "        category = \"general\"\n",
        "\n",
        "        times = []\n",
        "        for mm in msgs:\n",
        "            ts = _to_ts_or_none(mm.get(\"created_at\", \"\"))\n",
        "            if ts:\n",
        "                times.append(ts)\n",
        "        created_at = min(times) if times else None\n",
        "        updated_at = max(times) if times else None\n",
        "\n",
        "        batch.append({\n",
        "            \"id\": msg_ticket_id,\n",
        "            \"user_id\": DEFAULT_USER_ID,\n",
        "            \"title\": title,\n",
        "            \"category\": category,\n",
        "            \"description\": description or \"Caso general\",\n",
        "            \"status\": \"open\",\n",
        "            \"created_at\": created_at,\n",
        "            \"updated_at\": updated_at,\n",
        "        })\n",
        "\n",
        "    return batch\n",
        "\n",
        "def build_message_batch(messages_csv_rows: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "    out = []\n",
        "    for m in messages_csv_rows:\n",
        "        msg_id = (m.get(\"id\") or \"\").strip()\n",
        "        try:\n",
        "            msg_id = str(uuid.UUID(msg_id))\n",
        "        except Exception:\n",
        "            msg_id = str(uuid.uuid4())\n",
        "\n",
        "        ticket_id = _coerce_uuid_str(m.get(\"ticket_id\", \"\"))\n",
        "        is_bot = str(m.get(\"is_bot\", \"\")).strip().lower() in (\"true\",\"t\",\"1\",\"yes\",\"y\",\"si\",\"sí\")\n",
        "        sender_name = (m.get(\"sender_name\") or (\"Demo Kavak\" if is_bot else \"cliente\"))[:255]\n",
        "        content = (m.get(\"content\") or \"\").strip()\n",
        "        created_at = _to_ts_or_none(m.get(\"created_at\", \"\"))\n",
        "\n",
        "        out.append({\n",
        "            \"id\": msg_id,\n",
        "            \"ticket_id\": ticket_id,\n",
        "            \"content\": content,\n",
        "            \"is_bot\": is_bot,\n",
        "            \"sender_name\": sender_name,\n",
        "            \"created_at\": created_at,\n",
        "        })\n",
        "    return out\n",
        "\n",
        "# ============== SQL templates ==============\n",
        "SQL_TICKET = text(\"\"\"\n",
        "    INSERT INTO public.tickets\n",
        "        (id, user_id, title, category, description, status, created_at, updated_at)\n",
        "    VALUES\n",
        "        (:id, :user_id, :title, :category, :description, :status,\n",
        "         COALESCE(:created_at, CURRENT_TIMESTAMP),\n",
        "         COALESCE(:updated_at, CURRENT_TIMESTAMP))\n",
        "    ON CONFLICT (id) DO NOTHING\n",
        "\"\"\")\n",
        "\n",
        "# Si habilitas deduplicación, primero crea el índice único:\n",
        "# ALTER TABLE public.messages\n",
        "# ADD CONSTRAINT messages_unique_ticket_content_created_at UNIQUE (ticket_id, content, created_at);\n",
        "SQL_MESSAGE = text(\"\"\"\n",
        "    INSERT INTO public.messages\n",
        "        (id, ticket_id, content, is_bot, sender_name, created_at)\n",
        "    VALUES\n",
        "        (:id, :ticket_id, :content, :is_bot, :sender_name, COALESCE(:created_at, CURRENT_TIMESTAMP))\n",
        "\"\"\" if not DEDUP_MESSAGES else \"\"\"\n",
        "    INSERT INTO public.messages\n",
        "        (id, ticket_id, content, is_bot, sender_name, created_at)\n",
        "    VALUES\n",
        "        (:id, :ticket_id, :content, :is_bot, :sender_name, COALESCE(:created_at, CURRENT_TIMESTAMP))\n",
        "    ON CONFLICT (ticket_id, content, created_at) DO NOTHING\n",
        "\"\"\")\n",
        "\n",
        "# ============== Main ETL ==============\n",
        "def etl_upload_conversations(\n",
        "    database_url: str = DATABASE_URL,\n",
        "    meta_csv: str = META_CSV,\n",
        "    messages_csv: str = MESSAGES_CSV,\n",
        "    chunk_size: int = CHUNK_SIZE,\n",
        "):\n",
        "    # 1) Cargar CSVs\n",
        "    meta_rows = _load_csv_as_dicts(meta_csv)\n",
        "    msg_rows  = _load_csv_as_dicts(messages_csv)\n",
        "\n",
        "    # 2) Normalizar/Indexar mensajes por ticket (para calcular timestamps del ticket)\n",
        "    #    Importante: messages.csv ya debería tener ticket_id UUID (si usaste tu generador)\n",
        "    #    De todas formas, normalizamos a UUID por si acaso.\n",
        "    messages_by_ticket: Dict[str, List[Dict[str, Any]]] = {}\n",
        "    for m in msg_rows:\n",
        "        tid = _coerce_uuid_str(m.get(\"ticket_id\", \"\"))\n",
        "        m[\"ticket_id\"] = tid\n",
        "        messages_by_ticket.setdefault(tid, []).append(m)\n",
        "\n",
        "    # 3) Construir lotes para insert\n",
        "    ticket_batch   = build_ticket_batch(meta_rows, messages_by_ticket)\n",
        "        # Sanity-check: 1 ticket por conversación\n",
        "    # Cuenta conversaciones únicas en meta y tickets a insertar\n",
        "    unique_conversations = { (m.get(\"conversation_id\") or \"\").strip() for m in meta_rows }\n",
        "    unique_conversations = { c for c in unique_conversations if c }  # quita vacíos\n",
        "    expected_min_tickets = len(unique_conversations)\n",
        "\n",
        "    # Nota: ticket_batch incluye además los tickets \"fallback\" detectados desde mensajes\n",
        "    print(f\"ℹ️  Conversaciones únicas en meta: {expected_min_tickets} | Tickets a insertar: {len(ticket_batch)}\")\n",
        "    message_batch  = build_message_batch(msg_rows)\n",
        "\n",
        "    # 4) Insertar en DB por lotes (transaccional por chunk)\n",
        "    engine = sqlalchemy.create_engine(\n",
        "        database_url,\n",
        "        poolclass=NullPool,\n",
        "        connect_args={\"sslmode\": \"require\", \"connect_timeout\": 10},\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        with engine.begin() as conn:\n",
        "            if DISABLE_METRICS:\n",
        "                # Deshabilita SOLO este trigger\n",
        "                #conn.execute(text(\"ALTER TABLE public.messages DISABLE TRIGGER trg_update_chatbot_metrics\"))\n",
        "                pass\n",
        "        # ===== INSERTA TICKETS =====\n",
        "        total_tickets = len(ticket_batch)\n",
        "        if total_tickets:\n",
        "            iterator = _chunked(ticket_batch, chunk_size)\n",
        "            with engine.begin() as conn:\n",
        "                for chunk in iterator:\n",
        "                    conn.execute(SQL_TICKET, chunk)\n",
        "\n",
        "        # ===== INSERTA MENSAJES =====\n",
        "        total_msgs = len(message_batch)\n",
        "        if total_msgs:\n",
        "            iterator = _chunked(message_batch, chunk_size)\n",
        "            with engine.begin() as conn:\n",
        "                for chunk in iterator:\n",
        "                    conn.execute(SQL_MESSAGE, chunk)\n",
        "\n",
        "    finally:\n",
        "        # Rehabilita el trigger pase lo que pase\n",
        "        #with engine.begin() as conn:\n",
        "        #    if DISABLE_METRICS:\n",
        "        #        conn.execute(text(\"ALTER TABLE public.messages ENABLE TRIGGER trg_update_chatbot_metrics\"))\n",
        "        engine.dispose()\n",
        "\n",
        "    print(f\"✅ ETL OK: {total_tickets} tickets y {total_msgs} mensajes insertados (trigger de métricas {'desactivado' if DISABLE_METRICS else 'activo'}).\")\n",
        "DISABLE_METRICS = True  # <--- bandera\n",
        "\n",
        "\n",
        "# ============== CLI simple ==============\n",
        "if __name__ == \"__main__\":\n",
        "    #print(f\"DB: {DATABASE_URL}\")\n",
        "    print(f\"META_CSV: {META_CSV}\")\n",
        "    print(f\"MESSAGES_CSV: {MESSAGES_CSV}\")\n",
        "    etl_upload_conversations()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5yN2ARXx8Qu",
        "outputId": "e583a5c4-cd36-42af-e198-f93a11053a03"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DB: postgresql://postgres.nqfzagaxtsckofbuylra:HackathonKavak01!@aws-1-us-east-1.pooler.supabase.com:5432/postgres\n",
            "META_CSV: synthetic_kavak/conversations_meta.csv\n",
            "MESSAGES_CSV: synthetic_kavak/messages.csv\n",
            "ℹ️  Conversaciones únicas en meta: 97 | Tickets a insertar: 169\n",
            "✅ ETL OK: 169 tickets y 537 mensajes insertados (trigger de métricas desactivado).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_tool_reco.py\n",
        "# ------------------------------------------------------------\n",
        "# 1) lee conversaciones JSONL (formato Kavak que ya usas)\n",
        "# 2) detecta intents/pain-points -> propone \"tools\" internos\n",
        "# 3) sugiere cambios de código (patch ideas)\n",
        "# 4) evalúa y rankea con métricas configurables\n",
        "# 5) exporta CSV y reporte Markdown\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import json, re, os, math, uuid\n",
        "from collections import Counter, defaultdict\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "# Ajusta estas rutas a conveniencia\n",
        "INPUT_PATH = os.getenv(\"INPUT_PATH\", \"/content/synthetic_kavak/conversations.jsonl\")\n",
        "OUT_DIR = Path(os.getenv(\"OUT_DIR\", \"./agent_tool_reco_out\"))\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Pesos de métricas para el score (ajusta libremente)\n",
        "METRIC_WEIGHTS = {\n",
        "    \"frequency\": 0.35,        # frecuencia del intent\n",
        "    \"unresolved_rate\": 0.30,  # proporción no resueltas\n",
        "    \"csat_gap\": 0.20,         # brecha vs target csat\n",
        "    \"effort_inverse\": 0.15,   # menor esfuerzo => mayor score\n",
        "}\n",
        "CSAT_TARGET = float(os.getenv(\"CSAT_TARGET\", \"4.5\"))\n",
        "\n",
        "# Patrones de intención (seed para Kavak)\n",
        "INTENT_PATTERNS = {\n",
        "    \"offer_24h\": [r\"oferta.*24\", r\"offer.*24\"],\n",
        "    \"status_eval\": [r\"evaluaci[oó]n mec[aá]nica\", r\"status.*(eval|inspection)\", r\"estado.*inspecci[oó]n\"],\n",
        "    \"payment_status\": [r\"pago(s)?\", r\"transferencia\", r\"deposit(o|ó)\"],\n",
        "    \"reschedule_inspection\": [r\"reprogram(ar|aci[oó]n).*(inspecci[oó]n|visita)\", r\"cambiar.*cita\"],\n",
        "    \"credit_prequal\": [r\"cr[eé]dito\", r\"tasa(s)?\", r\"financ(i|)amiento\", r\"pre(-| )?aprobaci[oó]n\"],\n",
        "    \"warranty_claim\": [r\"garant[ií]a\", r\"falla el[eé]ctrica\", r\"claim|reclamo.*garant[ií]a\"],\n",
        "    \"kyc_docs\": [r\"document(o|os|aci[oó]n)\", r\"KYC\", r\"identificaci[oó]n\", r\"comprobante\"],\n",
        "    \"appointment\": [r\"(cita|agendar|programar)\"],\n",
        "    \"vin_vehicle_data\": [r\"VIN\", r\"placa(s)?\", r\"n[uú]mero.*serie\"],\n",
        "    \"complaint_compensation\": [r\"queja|reclamo\", r\"compensaci[oó]n|compensation\"],\n",
        "}\n",
        "\n",
        "# Esfuerzo estimado (1 fácil, 3 difícil).  Menor esfuerzo => mayor prioridad ceteris paribus\n",
        "EFFORT_TABLE = {\n",
        "    \"offer_24h\": 3,\n",
        "    \"status_eval\": 2,\n",
        "    \"payment_status\": 2,\n",
        "    \"reschedule_inspection\": 1,\n",
        "    \"credit_prequal\": 3,\n",
        "    \"warranty_claim\": 3,\n",
        "    \"kyc_docs\": 1,\n",
        "    \"appointment\": 1,\n",
        "    \"vin_vehicle_data\": 1,\n",
        "    \"complaint_compensation\": 2,\n",
        "}\n",
        "\n",
        "# Mapeo intent -> nombre de tool sugerida\n",
        "INTENT_TO_TOOL = {\n",
        "    \"offer_24h\": \"OfferIn24 Orchestrator\",\n",
        "    \"status_eval\": \"Inspection/Workshop Status Tracker\",\n",
        "    \"payment_status\": \"Payout Status Tracker\",\n",
        "    \"reschedule_inspection\": \"Inspection Re-Scheduler\",\n",
        "    \"credit_prequal\": \"Credit Pre-Qualification Simulator\",\n",
        "    \"warranty_claim\": \"Warranty Coverage Checker\",\n",
        "    \"kyc_docs\": \"Doc & KYC Collector\",\n",
        "    \"appointment\": \"Scheduling Assistant\",\n",
        "    \"vin_vehicle_data\": \"Vehicle Data Normalizer (VIN/plates)\",\n",
        "    \"complaint_compensation\": \"Retention & Compensation Playbook\",\n",
        "}\n",
        "\n",
        "# ===================== IO =====================\n",
        "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                # parchea comas al final si las hubiera\n",
        "                try:\n",
        "                    line2 = re.sub(r\",\\s*}\", \"}\", line)\n",
        "                    line2 = re.sub(r\",\\s*]\", \"]\", line2)\n",
        "                    rows.append(json.loads(line2))\n",
        "                except:\n",
        "                    pass\n",
        "    return rows\n",
        "\n",
        "# ===================== EXTRACCIÓN =====================\n",
        "def extract_text_blocks(conv: Dict[str, Any]) -> Tuple[str, List[str]]:\n",
        "    transcript = conv.get(\"transcript\", [])\n",
        "    blocks = []\n",
        "    for t in transcript:\n",
        "        txt = (t.get(\"text\") or \"\").strip()\n",
        "        if txt:\n",
        "            blocks.append(txt.lower())\n",
        "    combined = \" \".join(blocks)\n",
        "    return combined, blocks\n",
        "\n",
        "def detect_intents(text: str) -> List[str]:\n",
        "    intents = set()\n",
        "    for intent, patterns in INTENT_PATTERNS.items():\n",
        "        for pat in patterns:\n",
        "            if re.search(pat, text, re.IGNORECASE):\n",
        "                intents.add(intent)\n",
        "                break\n",
        "    return sorted(list(intents))\n",
        "\n",
        "def conv_metrics(conv: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    meta = conv.get(\"meta\", {}) or {}\n",
        "    outcomes = conv.get(\"outcomes\", {}) or {}\n",
        "    resolved = bool(meta.get(\"resolved\", False))\n",
        "    csat = outcomes.get(\"csat_estimated_1_5\")\n",
        "    try:\n",
        "        csat = float(csat) if csat is not None else None\n",
        "    except:\n",
        "        csat = None\n",
        "    return {\"resolved\": resolved, \"csat\": csat}\n",
        "\n",
        "# ===================== PIPELINE =====================\n",
        "def build_ranking(convs: List[Dict[str, Any]]) -> pd.DataFrame:\n",
        "    intent_stats = defaultdict(lambda: {\n",
        "        \"count\": 0,\n",
        "        \"unresolved\": 0,\n",
        "        \"csat_sum\": 0.0,\n",
        "        \"csat_n\": 0,\n",
        "        \"contexts\": Counter(),\n",
        "    })\n",
        "    per_conv = []\n",
        "\n",
        "    for conv in convs:\n",
        "        text_all, _ = extract_text_blocks(conv)\n",
        "        intents = detect_intents(text_all)\n",
        "        m = conv_metrics(conv)\n",
        "        context = (conv.get(\"meta\", {}).get(\"context\") or \"unknown\").lower()\n",
        "\n",
        "        for it in intents:\n",
        "            st = intent_stats[it]\n",
        "            st[\"count\"] += 1\n",
        "            st[\"contexts\"][context] += 1\n",
        "            if not m[\"resolved\"]:\n",
        "                st[\"unresolved\"] += 1\n",
        "            if m[\"csat\"] is not None:\n",
        "                st[\"csat_sum\"] += m[\"csat\"]\n",
        "                st[\"csat_n\"] += 1\n",
        "\n",
        "        per_conv.append({\n",
        "            \"conversation_id\": conv.get(\"meta\", {}).get(\"conversation_id\", \"\"),\n",
        "            \"context\": context,\n",
        "            \"intents\": intents,\n",
        "            \"resolved\": m[\"resolved\"],\n",
        "            \"csat\": m[\"csat\"],\n",
        "        })\n",
        "\n",
        "    # construir dataframe de candidatos\n",
        "    rows = []\n",
        "    for intent, st in intent_stats.items():\n",
        "        if st[\"count\"] == 0:\n",
        "            continue\n",
        "        freq = st[\"count\"]\n",
        "        unresolved_rate = st[\"unresolved\"] / max(1, st[\"count\"])\n",
        "        avg_csat = (st[\"csat_sum\"] / st[\"csat_n\"]) if st[\"csat_n\"] > 0 else None\n",
        "        csat_gap = (CSAT_TARGET - avg_csat) if avg_csat is not None else 0.5\n",
        "        effort = EFFORT_TABLE.get(intent, 3)\n",
        "        effort_inverse = 1.0 / effort\n",
        "        tool_name = INTENT_TO_TOOL.get(intent, f\"Tool for {intent}\")\n",
        "        rows.append({\n",
        "            \"intent\": intent,\n",
        "            \"tool_name\": tool_name,\n",
        "            \"frequency\": freq,\n",
        "            \"unresolved_rate\": unresolved_rate,\n",
        "            \"avg_csat\": avg_csat,\n",
        "            \"csat_gap\": csat_gap,\n",
        "            \"effort_est\": effort,\n",
        "            \"effort_inverse\": effort_inverse,\n",
        "            \"top_contexts\": \", \".join([f\"{k}:{v}\" for k,v in intent_stats[intent][\"contexts\"].most_common(3)]),\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    if df.empty:\n",
        "        return df\n",
        "\n",
        "    # normalización min-max\n",
        "    def norm(series: pd.Series) -> pd.Series:\n",
        "        s = series.astype(float)\n",
        "        if s.nunique() == 1:\n",
        "            return pd.Series([0.5]*len(s), index=s.index)\n",
        "        mn, mx = s.min(), s.max()\n",
        "        if mx == mn:\n",
        "            return s - mn\n",
        "        return (s - mn) / (mx - mn)\n",
        "\n",
        "    df[\"frequency_norm\"]  = norm(df[\"frequency\"])\n",
        "    df[\"unresolved_norm\"] = norm(df[\"unresolved_rate\"])\n",
        "    df[\"csat_gap_norm\"]   = norm(df[\"csat_gap\"])\n",
        "    df[\"effort_inv_norm\"] = norm(df[\"effort_inverse\"])\n",
        "\n",
        "    df[\"score\"] = (\n",
        "        METRIC_WEIGHTS[\"frequency\"]       * df[\"frequency_norm\"] +\n",
        "        METRIC_WEIGHTS[\"unresolved_rate\"] * df[\"unresolved_norm\"] +\n",
        "        METRIC_WEIGHTS[\"csat_gap\"]        * df[\"csat_gap_norm\"] +\n",
        "        METRIC_WEIGHTS[\"effort_inverse\"]  * df[\"effort_inv_norm\"]\n",
        "    )\n",
        "\n",
        "    df = df.sort_values(\"score\", ascending=False)\n",
        "    return df\n",
        "\n",
        "def build_code_suggestions(df: pd.DataFrame) -> List[Dict[str, str]]:\n",
        "    suggestions = []\n",
        "\n",
        "    def add(title, rationale, patch):\n",
        "        suggestions.append({\"title\": title, \"rationale\": rationale, \"patch\": patch})\n",
        "\n",
        "    if not df.empty and (df[\"intent\"] == \"kyc_docs\").any():\n",
        "        add(\n",
        "            \"Extractor/validador KYC con checklist\",\n",
        "            \"Alta frecuencia de solicitudes de documentos y posibles rechazos. Un validador reduce reprocesos y TAT.\",\n",
        "            \"\"\"# kyc.py\n",
        "# validate_kyc(payload) -> {'ok': bool, 'missing':[], 'rejected':[{doc,reason}]}\n",
        "# - REQUIRED_DOCS por país\n",
        "# - OCR básico para legibilidad\n",
        "# - razones de rechazo estándares\n",
        "\"\"\",\n",
        "        )\n",
        "\n",
        "    if not df.empty and (df[\"intent\"] == \"status_eval\").any():\n",
        "        add(\n",
        "            \"Endpoint /inspection/status + webhook\",\n",
        "            \"Muchas consultas repetidas sobre estado de evaluación. Proveer endpoint y notificaciones proactivas.\",\n",
        "            \"\"\"# GET /inspection/status?ticket_id=...\n",
        "# -> {'stage':'in_review','eta_hours':24,'last_update':...}\n",
        "# Webhook 'inspection.progress' para pushes y reducir costos de soporte.\n",
        "\"\"\",\n",
        "        )\n",
        "\n",
        "    if not df.empty and (df[\"intent\"] == \"reschedule_inspection\").any():\n",
        "        add(\n",
        "            \"Integración de agenda para reprogramación automática\",\n",
        "            \"Reagendar aparece frecuente: exponer slots y confirmar en 1 clic.\",\n",
        "            \"\"\"# GET /appointments/availability?zip=...&date=...\n",
        "# POST /appointments/reschedule {ticket_id, slot_id}\n",
        "\"\"\",\n",
        "        )\n",
        "\n",
        "    # Sugerencias generales basadas en incidencias típicas del pipeline\n",
        "    add(\n",
        "        \"UUID determinístico por conversación (uuid5)\",\n",
        "        \"Evita desalineos ticket_id/messages y simplifica re-cargas idempotentes.\",\n",
        "        \"\"\"# ensure_defaults():\n",
        "# meta.setdefault(\"conversation_id\", str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{contexto}:{seed or ''}\")))\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    add(\n",
        "        \"Feature flag para desactivar métricas en ETL\",\n",
        "        \"Evita errores por triggers al cargar masivo; control por sesión.\",\n",
        "        \"\"\"-- SQL:\n",
        "CREATE OR REPLACE FUNCTION public.update_chatbot_metrics() RETURNS trigger AS $$\n",
        "BEGIN\n",
        "  IF current_setting('app.enable_metrics', true) = 'off' THEN RETURN NEW; END IF;\n",
        "  -- resto...\n",
        "END; $$ LANGUAGE plpgsql;\n",
        "\n",
        "-- ETL:\n",
        "SELECT set_config('app.enable_metrics','off', true);\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    add(\n",
        "        \"Deduplicación de mensajes por (ticket_id, content, created_at)\",\n",
        "        \"Permite re-ejecutar ETLs sin duplicar mensajes idénticos.\",\n",
        "        \"\"\"ALTER TABLE public.messages\n",
        "ADD CONSTRAINT messages_unique_ticket_content_created_at\n",
        "UNIQUE (ticket_id, content, created_at);\n",
        "\n",
        "-- Inserción:\n",
        "... ON CONFLICT (ticket_id, content, created_at) DO NOTHING;\n",
        "\"\"\",\n",
        "    )\n",
        "\n",
        "    return suggestions\n",
        "\n",
        "def write_outputs(df: pd.DataFrame, convs_count: int, out_dir: Path) -> Tuple[str, str]:\n",
        "    csv_path = out_dir / \"candidate_tools_ranked.csv\"\n",
        "    md_path = out_dir / \"report.md\"\n",
        "\n",
        "    df.to_csv(csv_path, index=False)\n",
        "\n",
        "    md = []\n",
        "    md.append(\"# Tooling Recommendations Report\\n\")\n",
        "    md.append(f\"- Conversations processed: {convs_count}\")\n",
        "    md.append(f\"- Unique intents detected: {len(df)}\\n\")\n",
        "\n",
        "    if not df.empty:\n",
        "        md.append(\"## Top Candidate Tools\\n\")\n",
        "        for _, r in df.head(10).iterrows():\n",
        "            md.append(\n",
        "                f\"### {r['tool_name']}\\n\"\n",
        "                f\"- Intent: `{r['intent']}`\\n\"\n",
        "                f\"- Frequency: {int(r['frequency'])}\\n\"\n",
        "                f\"- Unresolved rate: {r['unresolved_rate']:.2f}\\n\"\n",
        "                f\"- Avg CSAT: {('%.2f' % r['avg_csat']) if pd.notnull(r['avg_csat']) else 'n/a'}\\n\"\n",
        "                f\"- Effort (1=low,3=high): {int(r['effort_est'])}\\n\"\n",
        "                f\"- Context hotspots: {r['top_contexts']}\\n\"\n",
        "                f\"- Score: {r['score']:.3f}\\n\"\n",
        "            )\n",
        "    else:\n",
        "        md.append(\"_No intents detected with current patterns. Adjust INTENT_PATTERNS._\\n\")\n",
        "\n",
        "    md.append(\"\\n## Code Change Suggestions\\n\")\n",
        "    for s in build_code_suggestions(df):\n",
        "        md.append(f\"### {s['title']}\\n{s['rationale']}\\n```text\\n{s['patch'].strip()}\\n```\\n\")\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(md))\n",
        "\n",
        "    return str(csv_path), str(md_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "6Y_TwL6FENXi"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================== MAIN =====================\n",
        "if __name__ == \"__main__\":\n",
        "    # if not os.path.exists(INPUT_PATH):\n",
        "    #     raise FileNotFoundError(f\"No existe INPUT_PATH: {INPUT_PATH}\")\n",
        "    INPUT_PATH = \"/content/synthetic_kavak/conversations.jsonl\"\n",
        "    convs = read_jsonl(INPUT_PATH)\n",
        "    df = build_ranking(convs)\n",
        "    csv_out, md_out = write_outputs(df, len(convs), OUT_DIR)\n",
        "\n",
        "    print(\"CSV:\", csv_out)\n",
        "    print(\"MD: \", md_out)\n",
        "    print(\"Conversations:\", len(convs))\n",
        "    print(\"Detected intents:\", 0 if df is None else len(df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4PJh5RpEUfg",
        "outputId": "69ce9201-5b6d-4671-9580-afd9c28c79fa"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV: agent_tool_reco_out/candidate_tools_ranked.csv\n",
            "MD:  agent_tool_reco_out/report.md\n",
            "Conversations: 102\n",
            "Detected intents: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_rewriter.py\n",
        "# ------------------------------------------------------------\n",
        "# Un único LLM:\n",
        "# - Analiza conversaciones (JSONL estilo Kavak)\n",
        "# - Propone cambios en PROMPT (system/user) y en CÓDIGO (parches)\n",
        "# - Sugiere nuevas \"tools\" internas (APIs/servicios) si aplican\n",
        "# - Devuelve plan de evaluación con métricas configurables\n",
        "# - Exporta resultados en JSON/MD\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os, json, re, uuid, math\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from collections import Counter, defaultdict\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm.auto import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "\n",
        "load_dotenv()\n",
        "# Usa la misma forma que tu código anterior:\n",
        "#   client = OpenAI(api_key=openai_kavak_secret)\n",
        "# (si lo tienes en tu entorno)\n",
        "try:\n",
        "    client = OpenAI(api_key=openai_kavak_secret)  # noqa\n",
        "except NameError:\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-nano-2025-08-07\")  # usa el que tengas disponible\n",
        "\n",
        "# Rutas\n",
        "INPUT_PATH = os.getenv(\"INPUT_PATH\", \"synthetic_kavak/conversations.jsonl\")\n",
        "OUT_DIR = Path(os.getenv(\"OUT_DIR\", \"agent_rewriter_out\"))\n",
        "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Métricas objetivo ajustables\n",
        "EVAL_CONFIG = {\n",
        "    \"target_csat\": float(os.getenv(\"TARGET_CSAT\", \"4.5\")),\n",
        "    \"max_turns\": int(os.getenv(\"TARGET_MAX_TURNS\", \"6\")),\n",
        "    \"resolution_rate_min\": float(os.getenv(\"TARGET_RESOLUTION_RATE\", \"0.8\")),\n",
        "    \"latency_secs_max\": int(os.getenv(\"TARGET_LATENCY_SECS\", \"120\")),\n",
        "    # pesos relativos (no tienen que sumar 1)\n",
        "    \"weights\": {\n",
        "        \"csat\": 0.35,\n",
        "        \"resolution_rate\": 0.35,\n",
        "        \"turns_efficiency\": 0.15,\n",
        "        \"latency\": 0.15,\n",
        "    },\n",
        "}\n",
        "\n",
        "# Patrones de intención (semilla) — puedes extenderlos\n",
        "INTENT_PATTERNS = {\n",
        "    \"offer_24h\": [r\"oferta.*24\", r\"offer.*24\"],\n",
        "    \"status_eval\": [r\"evaluaci[oó]n mec[aá]nica\", r\"status.*(eval|inspection)\", r\"estado.*inspecci[oó]n\"],\n",
        "    \"payment_status\": [r\"pago(s)?\", r\"transferencia\", r\"deposit(o|ó)\"],\n",
        "    \"reschedule_inspection\": [r\"reprogram(ar|aci[oó]n).*(inspecci[oó]n|visita)\", r\"cambiar.*cita\"],\n",
        "    \"credit_prequal\": [r\"cr[eé]dito\", r\"tasa(s)?\", r\"financ(i|)amiento\", r\"pre(-| )?aprobaci[oó]n\"],\n",
        "    \"warranty_claim\": [r\"garant[ií]a\", r\"falla el[eé]ctrica\"],\n",
        "    \"kyc_docs\": [r\"document(o|os|aci[oó]n)\", r\"KYC\", r\"identificaci[oó]n\", r\"comprobante\"],\n",
        "    \"appointment\": [r\"(cita|agendar|programar)\"],\n",
        "}\n",
        "\n",
        "# Heurística simple para seleccionar ejemplos relevantes (muestras para el LLM)\n",
        "SAMPLE_SIZES = {\n",
        "    \"worst_csat\": 8,\n",
        "    \"unresolved\": 8,\n",
        "    \"long_turns\": 6\n",
        "}\n",
        "\n",
        "# ===================== CARGA Y PRE-PROCESO =====================\n",
        "\n",
        "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                # parchea comas colgantes\n",
        "                try:\n",
        "                    line2 = re.sub(r\",\\s*}\", \"}\", line)\n",
        "                    line2 = re.sub(r\",\\s*]\", \"]\", line2)\n",
        "                    rows.append(json.loads(line2))\n",
        "                except:\n",
        "                    pass\n",
        "    return rows\n",
        "\n",
        "def lower(txt: str) -> str:\n",
        "    return (txt or \"\").lower()\n",
        "\n",
        "def detect_intents_text(text: str) -> List[str]:\n",
        "    intents = set()\n",
        "    for name, patterns in INTENT_PATTERNS.items():\n",
        "        for p in patterns:\n",
        "            if re.search(p, text, re.IGNORECASE):\n",
        "                intents.add(name)\n",
        "                break\n",
        "    return sorted(list(intents))\n",
        "\n",
        "def conv_to_row(conv: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    meta = conv.get(\"meta\", {}) or {}\n",
        "    outcomes = conv.get(\"outcomes\", {}) or {}\n",
        "    transcript = conv.get(\"transcript\", []) or []\n",
        "\n",
        "    # métricas por conversación\n",
        "    resolved = bool(meta.get(\"resolved\", False))\n",
        "    csat = outcomes.get(\"csat_estimated_1_5\")\n",
        "    try:\n",
        "        csat = float(csat) if csat is not None else None\n",
        "    except:\n",
        "        csat = None\n",
        "\n",
        "    turns = len(transcript)\n",
        "    duration = meta.get(\"duration_sec\") or 0\n",
        "\n",
        "    text_blocks = []\n",
        "    for t in transcript:\n",
        "        ttext = (t.get(\"text\") or \"\").strip()\n",
        "        if ttext:\n",
        "            text_blocks.append(ttext.lower())\n",
        "    combined = \" \".join(text_blocks)\n",
        "    intents = detect_intents_text(combined)\n",
        "\n",
        "    return {\n",
        "        \"conversation_id\": meta.get(\"conversation_id\", str(uuid.uuid4())),\n",
        "        \"context\": lower(meta.get(\"context\")),\n",
        "        \"resolved\": resolved,\n",
        "        \"csat\": csat,\n",
        "        \"turns\": turns,\n",
        "        \"duration_sec\": duration,\n",
        "        \"intents\": intents,\n",
        "        \"transcript\": transcript,\n",
        "        \"meta\": meta,\n",
        "        \"outcomes\": outcomes,\n",
        "    }\n",
        "\n",
        "def load_conversations(path: str) -> pd.DataFrame:\n",
        "    convs = read_jsonl(path)\n",
        "    rows = [conv_to_row(c) for c in convs]\n",
        "    df = pd.DataFrame(rows)\n",
        "    # valores por defecto\n",
        "    if \"csat\" not in df or df[\"csat\"].isna().all():\n",
        "        df[\"csat\"] = None\n",
        "    return df\n",
        "\n",
        "# ===================== SELECCIÓN DE MUESTRAS =====================\n",
        "\n",
        "def pick_samples(df: pd.DataFrame) -> Dict[str, List[Dict[str, Any]]]:\n",
        "    samples = {}\n",
        "\n",
        "    # Peor CSAT\n",
        "    worst = df.copy()\n",
        "    worst = worst[worst[\"csat\"].notna()]\n",
        "    worst = worst.sort_values(\"csat\", ascending=True).head(SAMPLE_SIZES[\"worst_csat\"])\n",
        "    samples[\"worst_csat\"] = worst.to_dict(orient=\"records\")\n",
        "\n",
        "    # No resueltas\n",
        "    unresolved = df[df[\"resolved\"] == False].head(SAMPLE_SIZES[\"unresolved\"])  # noqa\n",
        "    samples[\"unresolved\"] = unresolved.to_dict(orient=\"records\")\n",
        "\n",
        "    # Más turnos\n",
        "    long_turns = df.sort_values(\"turns\", ascending=False).head(SAMPLE_SIZES[\"long_turns\"])\n",
        "    samples[\"long_turns\"] = long_turns.to_dict(orient=\"records\")\n",
        "\n",
        "    return samples\n",
        "\n",
        "# ===================== PROMPTS LLM =====================\n",
        "\n",
        "def build_system_prompt() -> str:\n",
        "    return (\n",
        "        \"Eres un arquitecto de conversación y plataforma para Kavak. \"\n",
        "        \"Analizas registros de soporte/ventas y propones mejoras en prompt y código. \"\n",
        "        \"Siempre prioriza claridad, cumplimiento (KYC, garantías, crédito) y reducción de fricción. \"\n",
        "        \"Devuelve SIEMPRE un JSON válido según el esquema solicitado.\"\n",
        "    )\n",
        "\n",
        "def build_user_prompt(df: pd.DataFrame, samples: Dict[str, List[Dict[str, Any]]]) -> str:\n",
        "    # Métricas agregadas actuales\n",
        "    total = len(df)\n",
        "    resolution_rate = (df[\"resolved\"].sum() / total) if total else 0.0\n",
        "    avg_csat = round(df[\"csat\"].dropna().mean(), 3) if df[\"csat\"].notna().any() else None\n",
        "    avg_turns = round(df[\"turns\"].mean(), 2) if total else 0\n",
        "    avg_dur = round(df[\"duration_sec\"].mean(), 1) if total else 0\n",
        "\n",
        "    # Compactar muestras\n",
        "    def compact(record):\n",
        "        tr = record.get(\"transcript\", [])\n",
        "        # quedarnos con 3 turnos de ejemplo (si hay muchos)\n",
        "        tr_excerpt = tr[:3] if len(tr) > 3 else tr\n",
        "        return {\n",
        "            \"conversation_id\": record.get(\"conversation_id\"),\n",
        "            \"context\": record.get(\"context\"),\n",
        "            \"resolved\": record.get(\"resolved\"),\n",
        "            \"csat\": record.get(\"csat\"),\n",
        "            \"turns\": record.get(\"turns\"),\n",
        "            \"duration_sec\": record.get(\"duration_sec\"),\n",
        "            \"intents\": record.get(\"intents\"),\n",
        "            \"excerpt\": [{\"speaker\": t.get(\"speaker\"), \"text\": t.get(\"text\")} for t in tr_excerpt],\n",
        "        }\n",
        "\n",
        "    compacted = {k: [compact(r) for r in v] for k, v in samples.items()}\n",
        "\n",
        "    # Config de evaluación\n",
        "    eval_cfg = EVAL_CONFIG\n",
        "\n",
        "    # Esquema esperado\n",
        "    schema = {\n",
        "        \"type\": \"object\",\n",
        "        \"required\": [\"prompt_changes\", \"code_changes\", \"tools\", \"evaluation_plan\", \"risks\"],\n",
        "        \"properties\": {\n",
        "            \"prompt_changes\": {\n",
        "                \"type\": \"object\",\n",
        "                \"required\": [\"system_patch\", \"user_patch\", \"rationale\"],\n",
        "                \"properties\": {\n",
        "                    \"system_patch\": {\"type\": \"string\"},\n",
        "                    \"user_patch\": {\"type\": \"string\"},\n",
        "                    \"rationale\": {\"type\": \"string\"}\n",
        "                }\n",
        "            },\n",
        "            \"code_changes\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"required\": [\"title\", \"patch\", \"impact\", \"risk\"],\n",
        "                    \"properties\": {\n",
        "                        \"title\": {\"type\": \"string\"},\n",
        "                        \"patch\": {\"type\": \"string\"},\n",
        "                        \"impact\": {\"type\": \"string\"},\n",
        "                        \"risk\": {\"type\": \"string\"}\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"tools\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\n",
        "                    \"type\": \"object\",\n",
        "                    \"required\": [\"name\", \"why\", \"api_sketch\", \"effort_1_3\"],\n",
        "                    \"properties\": {\n",
        "                        \"name\": {\"type\": \"string\"},\n",
        "                        \"why\": {\"type\": \"string\"},\n",
        "                        \"api_sketch\": {\"type\": \"string\"},\n",
        "                        \"effort_1_3\": {\"type\": \"integer\", \"minimum\": 1, \"maximum\": 3}\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"evaluation_plan\": {\n",
        "                \"type\": \"object\",\n",
        "                \"required\": [\"metrics\", \"offline_protocol\", \"online_protocol\", \"success_criteria\"],\n",
        "                \"properties\": {\n",
        "                    \"metrics\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n",
        "                    \"offline_protocol\": {\"type\": \"string\"},\n",
        "                    \"online_protocol\": {\"type\": \"string\"},\n",
        "                    \"success_criteria\": {\"type\": \"string\"}\n",
        "                }\n",
        "            },\n",
        "            \"risks\": {\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {\"type\": \"string\"}\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"current_aggregates\": {\n",
        "            \"total_conversations\": total,\n",
        "            \"resolution_rate\": round(resolution_rate, 3),\n",
        "            \"avg_csat\": avg_csat,\n",
        "            \"avg_turns\": avg_turns,\n",
        "            \"avg_duration_sec\": avg_dur\n",
        "        },\n",
        "        \"samples\": compacted,\n",
        "        \"eval_config\": eval_cfg,\n",
        "        \"schema\": schema,\n",
        "        \"instructions\": (\n",
        "            \"Analiza los ejemplos y las métricas. \"\n",
        "            \"1) Propón cambios concretos al PROMPT (system/user) que reduzcan turnos y aumenten CSAT. \"\n",
        "            \"2) Propón cambios de CÓDIGO en formato de patch o snippet (Python/SQL/HTTP) listos para pegar. \"\n",
        "            \"3) Sugiere herramientas internas (APIs o servicios) que eliminen fricción (status, KYC, agenda, pagos). \"\n",
        "            \"4) Devuelve un plan de evaluación (offline+online) usando las métricas y pesos indicados. \"\n",
        "            \"5) Enumera riesgos operativos/legales y cómo mitigarlos breve. \"\n",
        "            \"Responde SOLO con el JSON que cumple el esquema.\"\n",
        "        )\n",
        "    }\n",
        "    return json.dumps(payload, ensure_ascii=False, indent=2)\n",
        "\n",
        "# ===================== LLAMADA AL LLM =====================\n",
        "\n",
        "def ask_llm(system_prompt: str, user_prompt: str) -> Dict[str, Any]:\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=1,\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\",   \"content\": user_prompt}\n",
        "        ]\n",
        "    )\n",
        "    content = (resp.choices[0].message.content or \"\").strip()\n",
        "\n",
        "    # a veces llegan ```json fences\n",
        "    if content.startswith(\"```\"):\n",
        "        content = content.strip(\"`\")\n",
        "        lines = content.splitlines()\n",
        "        if lines and lines[0].lower().startswith(\"json\"):\n",
        "            content = \"\\n\".join(lines[1:])\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"error\": \"Invalid JSON from model\", \"raw\": content}\n",
        "\n",
        "# ===================== EVALUADOR LOCAL (opcional) =====================\n",
        "\n",
        "def quick_offline_eval(df: pd.DataFrame, cfg: Dict[str, Any]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Eval simulado: devuelve score agregado con tus pesos.\n",
        "    (Aquí no aplica cambios aún; sirve como baseline.)\n",
        "    \"\"\"\n",
        "    total = len(df) or 1\n",
        "    res_rate = (df[\"resolved\"].sum() / total)\n",
        "    avg_csat = df[\"csat\"].dropna().mean() if df[\"csat\"].notna().any() else None\n",
        "    turns = df[\"turns\"].mean()\n",
        "    lat = df[\"duration_sec\"].mean()\n",
        "\n",
        "    # normalizaciones simples (0-1); puedes cambiarlas\n",
        "    def inv_norm(x, max_ref):\n",
        "        return max(0.0, min(1.0, 1.0 - (x / max_ref))) if max_ref > 0 else 0.5\n",
        "\n",
        "    w = cfg[\"weights\"]\n",
        "    score = 0.0\n",
        "    parts = {}\n",
        "\n",
        "    # csat: si no hay, 0.5 neutro\n",
        "    csat_comp = (avg_csat / cfg[\"target_csat\"]) if avg_csat else 0.5\n",
        "    parts[\"csat\"] = csat_comp * w[\"csat\"]; score += parts[\"csat\"]\n",
        "    parts[\"resolution_rate\"] = res_rate * w[\"resolution_rate\"]; score += parts[\"resolution_rate\"]\n",
        "    parts[\"turns_efficiency\"] = inv_norm(turns, cfg.get(\"max_turns\", 8)) * w[\"turns_efficiency\"]; score += parts[\"turns_efficiency\"]\n",
        "    parts[\"latency\"] = inv_norm(lat, cfg.get(\"latency_secs_max\", 180)) * w[\"latency\"]; score += parts[\"latency\"]\n",
        "\n",
        "    return {\"score\": round(score, 3), \"components\": parts,\n",
        "            \"baseline\": {\"resolution_rate\": round(res_rate,3), \"avg_csat\": (round(avg_csat,3) if avg_csat else None),\n",
        "                         \"avg_turns\": round(turns,2), \"avg_latency\": round(lat,1)}}\n",
        "\n",
        "# ===================== SALIDAS =====================\n",
        "\n",
        "def save_outputs(llm_json: Dict[str, Any], df: pd.DataFrame, out_dir: Path) -> Dict[str, str]:\n",
        "    json_path = out_dir / \"llm_rewrite_response.json\"\n",
        "    md_path = out_dir / \"llm_rewrite_summary.md\"\n",
        "\n",
        "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(llm_json, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # MD resumen legible\n",
        "    lines = []\n",
        "    lines.append(\"# LLM Rewrite Summary\\n\")\n",
        "    agg = {\n",
        "        \"total\": len(df),\n",
        "        \"res_rate\": round((df['resolved'].sum()/len(df)) if len(df) else 0.0, 3),\n",
        "        \"avg_csat\": round(df['csat'].dropna().mean(), 3) if df['csat'].notna().any() else None\n",
        "    }\n",
        "    lines.append(f\"- Conversations: {agg['total']}\")\n",
        "    lines.append(f\"- Resolution rate: {agg['res_rate']}\")\n",
        "    lines.append(f\"- Avg CSAT: {agg['avg_csat']}\\n\")\n",
        "\n",
        "    if \"prompt_changes\" in llm_json:\n",
        "        pc = llm_json[\"prompt_changes\"]\n",
        "        lines.append(\"## Prompt Changes\\n\")\n",
        "        lines.append(\"### System patch\\n\")\n",
        "        lines.append(\"```text\\n\"+ (pc.get(\"system_patch\") or \"\").strip() +\"\\n```\\n\")\n",
        "        lines.append(\"### User patch\\n\")\n",
        "        lines.append(\"```text\\n\"+ (pc.get(\"user_patch\") or \"\").strip() +\"\\n```\\n\")\n",
        "        lines.append(\"**Rationale:** \" + (pc.get(\"rationale\") or \"\") + \"\\n\")\n",
        "\n",
        "    if \"code_changes\" in llm_json:\n",
        "        lines.append(\"## Code Changes\\n\")\n",
        "        for ch in llm_json[\"code_changes\"]:\n",
        "            lines.append(f\"### {ch.get('title')}\\n\")\n",
        "            lines.append(\"```text\\n\"+ (ch.get(\"patch\") or \"\").strip() +\"\\n```\\n\")\n",
        "            lines.append(f\"*Impact:* {ch.get('impact')}\\n*Risk:* {ch.get('risk')}\\n\")\n",
        "\n",
        "    if \"tools\" in llm_json:\n",
        "        lines.append(\"## Proposed Tools\\n\")\n",
        "        for t in llm_json[\"tools\"]:\n",
        "            lines.append(f\"### {t.get('name')} (effort {t.get('effort_1_3')})\\n\")\n",
        "            lines.append(t.get(\"why\",\"\") + \"\\n\")\n",
        "            lines.append(\"```text\\n\" + (t.get(\"api_sketch\") or \"\").strip() + \"\\n```\\n\")\n",
        "\n",
        "    if \"evaluation_plan\" in llm_json:\n",
        "        ep = llm_json[\"evaluation_plan\"]\n",
        "        lines.append(\"## Evaluation Plan\\n\")\n",
        "        lines.append(\"**Metrics:** \" + \", \".join(ep.get(\"metrics\", [])))\n",
        "        lines.append(\"\\n**Offline:**\\n\" + ep.get(\"offline_protocol\",\"\"))\n",
        "        lines.append(\"\\n**Online:**\\n\" + ep.get(\"online_protocol\",\"\"))\n",
        "        lines.append(\"\\n**Success criteria:**\\n\" + ep.get(\"success_criteria\",\"\") + \"\\n\")\n",
        "\n",
        "    if \"risks\" in llm_json:\n",
        "        lines.append(\"## Risks & Mitigations\\n\")\n",
        "        for r in llm_json[\"risks\"]:\n",
        "            lines.append(\"- \" + r)\n",
        "\n",
        "    with open(md_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(lines))\n",
        "\n",
        "    return {\"json\": str(json_path), \"md\": str(md_path)}\n",
        "\n",
        "# ===================== MAIN =====================\n",
        "\n",
        "def run():\n",
        "    if not os.path.exists(INPUT_PATH):\n",
        "        raise FileNotFoundError(f\"INPUT_PATH no existe: {INPUT_PATH}\")\n",
        "\n",
        "    df = load_conversations(INPUT_PATH)\n",
        "    samples = pick_samples(df)\n",
        "\n",
        "    sp = build_system_prompt()\n",
        "    up = build_user_prompt(df, samples)\n",
        "    llm_json = ask_llm(sp, up)\n",
        "\n",
        "    # guarda outputs\n",
        "    paths = save_outputs(llm_json, df, OUT_DIR)\n",
        "\n",
        "    # baseline rápido\n",
        "    baseline = quick_offline_eval(df, EVAL_CONFIG)\n",
        "\n",
        "    print(\"✅ Listo.\")\n",
        "    print(\"JSON:\", paths[\"json\"])\n",
        "    print(\"MD:  \", paths[\"md\"])\n",
        "    print(\"Baseline:\", baseline)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6Cb02S2F_gQ",
        "outputId": "49fef2a1-6c1b-417b-e114-6476f35b49d3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Listo.\n",
            "JSON: agent_rewriter_out/llm_rewrite_response.json\n",
            "MD:   agent_rewriter_out/llm_rewrite_summary.md\n",
            "Baseline: {'score': np.float64(0.458), 'components': {'csat': np.float64(0.33322440087145966), 'resolution_rate': np.float64(0.10637254901960783), 'turns_efficiency': np.float64(0.01838235294117647), 'latency': 0.0}, 'baseline': {'resolution_rate': np.float64(0.304), 'avg_csat': np.float64(4.284), 'avg_turns': np.float64(5.26), 'avg_latency': np.float64(691.4)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "80QgoqZAHrYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "874435d9"
      },
      "source": [
        "file_path = '/content/agent_rewriter_out/llm_rewrite_summary.md'\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    markdown_content = f.read()"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FV1P0HNNJCOd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce7164c6"
      },
      "source": [
        "{{markdown_content}}"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g_g6VjjmJ1Ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cf7794e7",
        "outputId": "741255d4-f4dc-4dd5-8757-3c14b3668044"
      },
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(markdown_content))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# LLM Rewrite Summary\n\n- Conversations: 102\n- Resolution rate: 0.304\n- Avg CSAT: 4.284\n\n## Prompt Changes\n\n### System patch\n\n```text\nEres un asesor de soporte/ventas de Kavak orientado a reducir fricción y elevar CSAT manteniendo cumplimiento de KYC, garantías y crédito. Guía a los agentes para: (a) priorizar acciones seguras y trazables (KYC, verificación de pagos); (b) usar respuestas cortas y centradas en la siguiente acción segura (p. ej., generar enlace seguro para KYC, agendar inspección, consultar estado de pago) en lugar de pedir múltiples datos en una sola evidencia no verificada; (c) cuando el cliente solicite compensación por retraso, verificar elegibilidad y escalar conforme políticas internas; (d) cuando la intención sea venta/compra, entregar un resumen de pasos y entregar enlaces seguros para documentación; (e) redirigir a canales seguros y evitar compartir datos sensibles por chat. Mantén tono empático, claro y concreto; evita promesas no respaldadas y registra cada interacción relevante para KYC y pagos.\n```\n\n### User patch\n\n```text\nActualiza los prompts de usuario para facilitar una única acción segura por turno: por ejemplo, si el tema es KYC o pago, responde con: 'Para continuar, te envío un enlace seguro para subir documentos. ¿Qué folio/identificación necesitas registrar?'; si es para agendar inspección, pregunta por fecha/hora disponibles; si es para estado de pago, solicita folio de venta y últimos 4 dígitos de cuenta para verificar. Incluye un enlace seguro generado y explícito el tiempo de expiración. Proporciona pasos numerados y evita pedir información sensible sin canal seguro. Rasgos clave: claridad, brevedad, acción siguiente única, cumplimiento de KYC, y enlace seguro.\n```\n\n**Rationale:** Reducir turnos y fricción al guiar al cliente hacia una acción segura y definida en cada interacción. Al separar flujos (KYC, pagos, inspecciones) con enlaces seguros y pasos claros, aumentamos CSAT y tasa de resolución manteniendo cumplimiento.\n\n## Code Changes\n\n### Reply templating engine (Python) para respuestas de acción única y segura\n\n```text\ndef build_agent_reply(conversation, intents, context, user_id, case_id=None):\n    # Prioridad: KYC/documents y estado de pagos\n    if 'kyc_docs' in intents or 'payment_status' in intents:\n        link = generate_secure_upload_link(user_id, case_id or conversation.get('case_id'))\n        return {\n            'reply': f'Para continuar, te envía un enlace seguro para subir documentos: {link} (expira en 15 minutos). Después de revisar, te indicaré el estado de tu pago o tus documentos.',\n            'action': 'send_secure_link',\n            'link': link\n        }\n    if 'appointment' in intents:\n        return {\n            'reply': '¿Prefieres una inspección hoy o mañana? Por favor indica fecha y hora disponibles.',\n            'action': 'schedule_inspection'\n        }\n    return {\n        'reply': '¿Sobre qué tema necesitas ayuda ahora? Dime el folio y el tema para guiarte.',\n        'action': 'default'\n    }\n```\n\n*Impact:* Permite respuestas más rápidas y seguras, reduciendo turnos y asegurando que se compartan datos sensibles solo a través de enlaces cifrados.\n*Risk:* Dependencia de la generación de enlaces; requiere rotación de claves y monitoreo de expiración para evitar uso indebido.\n\n### SQL/DB snippet para extraer casos de alto riesgo de fricción (auditoría rápida)\n\n```text\n-- Extrae 50 conversaciones con CSAT bajo y duración alta para revisión manual\nSELECT conversation_id, context, csat, turns, duration_sec, resolved\nFROM samples.unresolved\nORDER BY csat ASC, duration_sec DESC\nLIMIT 50;\n```\n\n*Impact:* Facilita la identificación rápida de casos que necesitan intervención humana para mejorar procesos y prompts.\n*Risk:* Puede exponer datos sensibles si se ejecuta sin filtros de seguridad; usar solo con role-based access.\n\n### HTTP: endpoint para generar enlace seguro de KYC\n\n```text\nPOST /api/v1/secure-upload-link\nHost: api.kavak.internal\nAuthorization: Bearer <token>\nContent-Type: application/json\n{\"user_id\": \"USER_ID\", \"case_id\": \"CASE_ID\", \"purpose\": \"kyc_upload\"}\n\nResponse:\n{ \"url\": \"https://secure.kavak.internal/upload/abcdef\", \"expires_in\": 900 }\n```\n\n*Impact:* Automatiza generación de enlaces seguros para KYC, reduciendo exposición de datos sensibles en chat.\n*Risk:* Riesgo de uso indebido de enlaces; mitigación con tokens de un solo uso y expiración corta.\n\n## Proposed Tools\n\n### SecureLinkService (effort 2)\n\nGenera enlaces temporales para cargas de KYC/pagos sin exponer datos en chat.\n\n```text\nPOST /api/v1/secure-upload-link con body {user_id, case_id, purpose}. Devuelve {url, expires_in}. Usar TLS y tokens de acceso; registrar auditoría.\n```\n\n### PaymentsAPI (effort 2)\n\nConsultar estado de transferencia, plazos y reintentos automáticamente para reducir incertidumbre del cliente.\n\n```text\nGET /payments/{folio}/status; POST /payments/{folio}/escalate si retraso > SLA.\n```\n\n### SchedulingAPI (effort 2)\n\nAgendar inspecciones y reprogramaciones de forma automatizada con disponibilidad en tiempo real.\n\n```text\nPOST /inspections with payload {user_id, date, location, type}.\n```\n\n### KYCService (effort 2)\n\nVerificar identidad y titularidad de forma automatizada para acelerar casos de pago/compra.\n\n```text\nPOST /kyc/verify con {user_id, docs}.\n```\n\n### CSATForecastService (effort 1)\n\nPredice probabilidad de CSAT bajo para priorizar intervención humana temprana.\n\n```text\nPOST /csat/predict con {conversation_id}.\n```\n\n## Evaluation Plan\n\n**Metrics:** csat, resolution_rate, avg_turns, latency_secs\n\n**Offline:**\nCalibrar prompts con historial histórico: dividir en cohortes control/experimental, simular respuestas con prompts actualizados, medir CSAT y resolución en offline. Ajustar pesos de evaluación de acuerdo a resultados.\n\n**Online:**\nImplementar gradual rollout: 20% de tráfico con prompts mejorados durante 2 semanas; monitorizar CSAT, resolución, y turns; escalar si objetivos se alcanzan o revertir si caen por más de 5%.\n\n**Success criteria:**\nCSAT promedio >= 4.6; resolución >= 0.92; turns promedio <= 4; latency promedio <= 90 segundos; tasa de uso de enlaces seguros >= 80% en flujos KYC/pago.\n\n## Risks & Mitigations\n\n- Riesgo de fuga de datos PII si enlaces seguros no se gestionan correctamente (mitigación: tokens de un solo uso, expiran en 15 minutos, TLS, registro de auditoría).\n- Sobrecarga operativa si se generan demasiados enlaces y se pierden; mitigación: límites diarios por usuario y reintentos controlados.\n- Cumplimiento legal y KYC: automatizar sin revisión humana puede dejar casos incompletos; mitigación: reglas de validación mínima y escalamiento a analista cuando falla KYC.\n- Promesas de pago/compensación sin validar políticas podría generar incumplimientos; mitigación: consolidar wording con políticas oficiales y revisión de supervisores.\n- Riesgo de dependencia de APIs internas: disponibilidad y latencias; mitigación: circuit breakers, fallbacks, y observabilidad.\n- Cambio de políticas de créditos/garantías: evitar incoherencias entre ventas y créditos; mitigación: controles de negocio y revisión legal."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# agent_rewriter_apply.py\n",
        "# ------------------------------------------------------------\n",
        "# 1) Ejecuta el \"agent_rewriter\" para obtener parches de prompt/código\n",
        "# 2) Aplica los parches de PROMPT a un generador de conversaciones\n",
        "# 3) Re-genera dataset propuesto (paralelo) y compara métricas con baseline\n",
        "# 4) Exporta CSVs + informe comparativo en Markdown\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import os, json, uuid, random, csv, re, math, time\n",
        "from datetime import datetime, timedelta\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from tqdm.auto import tqdm\n",
        "from openai import OpenAI\n",
        "\n",
        "# ===================== CONFIG =====================\n",
        "load_dotenv()\n",
        "\n",
        "# Modelo y cliente\n",
        "MODEL = os.getenv(\"OPENAI_MODEL\", \"gpt-5-nano-2025-08-07\")\n",
        "try:\n",
        "    client = OpenAI(api_key=openai_kavak_secret)  # si lo usas así en tus scripts\n",
        "except NameError:\n",
        "    client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "\n",
        "# Fuentes de datos\n",
        "BASELINE_INPUT_JSONL = os.getenv(\"BASELINE_INPUT_JSONL\", \"synthetic_kavak/conversations.jsonl\")\n",
        "OUT_DIR = Path(os.getenv(\"OUT_DIR\", \"rewriter_apply_out\")); OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Volumen de regeneración propuesta\n",
        "PER_CONTEXT = int(os.getenv(\"PER_CONTEXT\", \"3\"))\n",
        "MAX_WORKERS = int(os.getenv(\"MAX_WORKERS\", \"8\"))\n",
        "MAX_ATTEMPTS = int(os.getenv(\"MAX_ATTEMPTS\", \"4\"))\n",
        "\n",
        "# Contextos/tonos/canales/idiomas (igual que tu generador)\n",
        "CONTEXTS = ['buying', 'ask', 'feedback', 'service', 'credit', 'warranty']\n",
        "TONOS = [\"amable\", \"empático\", \"formal\", \"resolutivo\", \"apologético\", \"directo\", \"entusiasta\"]\n",
        "CANALES = [\"whatsapp\", \"webchat\", \"email\", \"telefono\"]\n",
        "IDIOMAS = [\"es\", \"es\", \"es\", \"es\", \"en\"]\n",
        "\n",
        "# ===================== UTILIDADES =====================\n",
        "def read_jsonl(path: str) -> List[Dict[str, Any]]:\n",
        "    rows = []\n",
        "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            line=line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                rows.append(json.loads(line))\n",
        "            except json.JSONDecodeError:\n",
        "                try:\n",
        "                    line2 = re.sub(r\",\\s*}\", \"}\", line)\n",
        "                    line2 = re.sub(r\",\\s*]\", \"]\", line2)\n",
        "                    rows.append(json.loads(line2))\n",
        "                except:\n",
        "                    pass\n",
        "    return rows\n",
        "\n",
        "def metrics_from_convs(convs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    n = len(convs)\n",
        "    if n == 0:\n",
        "        return {\"total\": 0, \"resolution_rate\": 0, \"avg_csat\": None, \"avg_turns\": 0, \"avg_duration_sec\": 0}\n",
        "    resolved = 0\n",
        "    csats = []\n",
        "    turns = []\n",
        "    durs = []\n",
        "    for c in convs:\n",
        "        meta = c.get(\"meta\", {})\n",
        "        outc = c.get(\"outcomes\", {})\n",
        "        if meta.get(\"resolved\", False):\n",
        "            resolved += 1\n",
        "        cs = outc.get(\"csat_estimated_1_5\")\n",
        "        try:\n",
        "            cs = float(cs) if cs is not None else None\n",
        "        except:\n",
        "            cs = None\n",
        "        if cs is not None:\n",
        "            csats.append(cs)\n",
        "        turns.append(len(c.get(\"transcript\", [])))\n",
        "        durs.append(meta.get(\"duration_sec\", 0) or 0)\n",
        "    return {\n",
        "        \"total\": n,\n",
        "        \"resolution_rate\": round(resolved / n, 3),\n",
        "        \"avg_csat\": round(sum(csats)/len(csats), 3) if csats else None,\n",
        "        \"avg_turns\": round(sum(turns)/len(turns), 2),\n",
        "        \"avg_duration_sec\": round(sum(durs)/len(durs), 1),\n",
        "    }\n",
        "\n",
        "def save_jsonl(rows: List[Dict[str, Any]], path: str):\n",
        "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in rows:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "# ===================== 1) CORRER EL AGENTE (LLM) =====================\n",
        "# Reuso el agente del mensaje anterior, pero inline para que sea 1 script.\n",
        "\n",
        "def build_system_prompt() -> str:\n",
        "    return (\n",
        "        \"Eres un arquitecto de conversación y plataforma para Kavak. \"\n",
        "        \"Analizas registros de soporte/ventas y propones mejoras en prompt y código. \"\n",
        "        \"Siempre prioriza claridad, cumplimiento (KYC, garantías, crédito) y reducción de fricción. \"\n",
        "        \"Devuelve SIEMPRE un JSON válido según el esquema solicitado.\"\n",
        "    )\n",
        "\n",
        "def build_user_payload_for_llm(convs: List[Dict[str, Any]]) -> str:\n",
        "    # métricas globales\n",
        "    m = metrics_from_convs(convs)\n",
        "    # escoger muestras simples: peores csat / no resueltas / más turnos\n",
        "    def csat(c):\n",
        "        s = (c.get(\"outcomes\", {}) or {}).get(\"csat_estimated_1_5\")\n",
        "        try: return float(s) if s is not None else 999\n",
        "        except: return 999\n",
        "    worst = sorted([c for c in convs if (c.get(\"outcomes\", {}) or {}).get(\"csat_estimated_1_5\") is not None],\n",
        "                   key=csat)[:8]\n",
        "    unresolved = [c for c in convs if not (c.get(\"meta\", {}) or {}).get(\"resolved\", True)][:8]\n",
        "    long_turns = sorted(convs, key=lambda c: len(c.get(\"transcript\", [])), reverse=True)[:6]\n",
        "\n",
        "    def compact(c):\n",
        "        tr = c.get(\"transcript\", [])\n",
        "        ex = tr[:3] if len(tr) > 3 else tr\n",
        "        return {\n",
        "            \"conversation_id\": (c.get(\"meta\", {}) or {}).get(\"conversation_id\",\"\"),\n",
        "            \"context\": (c.get(\"meta\", {}) or {}).get(\"context\",\"\"),\n",
        "            \"resolved\": (c.get(\"meta\", {}) or {}).get(\"resolved\", False),\n",
        "            \"csat\": (c.get(\"outcomes\", {}) or {}).get(\"csat_estimated_1_5\"),\n",
        "            \"turns\": len(tr),\n",
        "            \"duration_sec\": (c.get(\"meta\", {}) or {}).get(\"duration_sec\", 0),\n",
        "            \"excerpt\": [{\"speaker\": t.get(\"speaker\"), \"text\": t.get(\"text\")} for t in ex]\n",
        "        }\n",
        "\n",
        "    payload = {\n",
        "        \"current_aggregates\": m,\n",
        "        \"samples\": {\n",
        "            \"worst_csat\": [compact(x) for x in worst],\n",
        "            \"unresolved\": [compact(x) for x in unresolved],\n",
        "            \"long_turns\": [compact(x) for x in long_turns],\n",
        "        },\n",
        "        \"schema\": {\n",
        "            \"type\":\"object\",\n",
        "            \"required\":[\"prompt_changes\",\"code_changes\",\"tools\",\"evaluation_plan\",\"risks\"],\n",
        "            \"properties\":{\n",
        "                \"prompt_changes\":{\n",
        "                    \"type\":\"object\",\n",
        "                    \"required\":[\"system_patch\",\"user_patch\",\"rationale\"],\n",
        "                    \"properties\":{\n",
        "                        \"system_patch\":{\"type\":\"string\"},\n",
        "                        \"user_patch\":{\"type\":\"string\"},\n",
        "                        \"rationale\":{\"type\":\"string\"}\n",
        "                    }},\n",
        "                \"code_changes\":{\"type\":\"array\",\"items\":{\n",
        "                    \"type\":\"object\",\"required\":[\"title\",\"patch\",\"impact\",\"risk\"],\n",
        "                    \"properties\":{\n",
        "                        \"title\":{\"type\":\"string\"},\n",
        "                        \"patch\":{\"type\":\"string\"},\n",
        "                        \"impact\":{\"type\":\"string\"},\n",
        "                        \"risk\":{\"type\":\"string\"}\n",
        "                    } }},\n",
        "                \"tools\":{\"type\":\"array\",\"items\":{\n",
        "                    \"type\":\"object\",\"required\":[\"name\",\"why\",\"api_sketch\",\"effort_1_3\"],\n",
        "                    \"properties\":{\n",
        "                        \"name\":{\"type\":\"string\"},\n",
        "                        \"why\":{\"type\":\"string\"},\n",
        "                        \"api_sketch\":{\"type\":\"string\"},\n",
        "                        \"effort_1_3\":{\"type\":\"integer\",\"minimum\":1,\"maximum\":3}\n",
        "                    } }},\n",
        "                \"evaluation_plan\":{\"type\":\"object\",\"required\":[\"metrics\",\"offline_protocol\",\"online_protocol\",\"success_criteria\"],\n",
        "                    \"properties\":{\n",
        "                        \"metrics\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}},\n",
        "                        \"offline_protocol\":{\"type\":\"string\"},\n",
        "                        \"online_protocol\":{\"type\":\"string\"},\n",
        "                        \"success_criteria\":{\"type\":\"string\"}\n",
        "                    }},\n",
        "                \"risks\":{\"type\":\"array\",\"items\":{\"type\":\"string\"}}\n",
        "            }\n",
        "        },\n",
        "        \"instructions\": (\n",
        "            \"Analiza y devuelve SOLO JSON que cumpla el esquema. \"\n",
        "            \"Incluye cambios concretos para PROMPT (system/user) y CÓDIGO con parches listos para pegar.\"\n",
        "        )\n",
        "    }\n",
        "    return json.dumps(payload, ensure_ascii=False)\n",
        "\n",
        "def run_llm_proposal(convs: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
        "    sp = build_system_prompt()\n",
        "    up = build_user_payload_for_llm(convs)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        #temperature=0.7,\n",
        "        response_format={\"type\":\"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\":sp},\n",
        "            {\"role\":\"user\",\"content\":up}\n",
        "        ]\n",
        "    )\n",
        "    content = (resp.choices[0].message.content or \"\").strip()\n",
        "    if content.startswith(\"```\"):\n",
        "        content = content.strip(\"`\")\n",
        "        lines = content.splitlines()\n",
        "        if lines and lines[0].lower().startswith(\"json\"):\n",
        "            content = \"\\n\".join(lines[1:])\n",
        "    try:\n",
        "        return json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        return {\"error\":\"Invalid JSON from model\",\"raw\":content}\n",
        "\n",
        "# ===================== 2) GENERADOR CON PROMPTS PLUGGABLES =====================\n",
        "\n",
        "def default_system_prompt() -> str:\n",
        "    return (\n",
        "        \"Eres un generador de conversaciones realistas de atención a clientes para Kavak. \"\n",
        "        \"Prioriza claridad, empatía y cumplimiento. No inventes datos sensibles. \"\n",
        "        \"Mantén diálogos breves y creíbles, en el idioma indicado.\"\n",
        "    )\n",
        "\n",
        "def default_user_prompt(contexto: str, tono: str, idioma: str, canal: str) -> str:\n",
        "    return f\"\"\"\n",
        "Crea una conversación breve entre **agente de Kavak** y **cliente**.\n",
        "- contexto: {contexto}\n",
        "- tono: {tono}\n",
        "- idioma: {\"español\" if idioma==\"es\" else \"inglés\"}\n",
        "- canal: {canal}\n",
        "\n",
        "Requisitos:\n",
        "1) Primer turno del **cliente**.\n",
        "2) Cumple políticas (documentos, inspección, pagos, garantías, crédito, KYC si aplica).\n",
        "3) Si no se resuelve, deja claro el siguiente paso (ticket, escalar, cita, docs).\n",
        "4) Respuestas concisas, naturales (no robóticas).\n",
        "5) Devuelve **solo un JSON** con claves: meta, transcript, outcomes.\n",
        "\n",
        "Estructura mínima:\n",
        "{{\n",
        "  \"meta\": {{\n",
        "    \"conversation_id\": \"\", \"company\": \"Kavak\", \"context\": \"{contexto}\",\n",
        "    \"channel\": \"{canal}\", \"tone\": \"{tono}\", \"language\": \"{idioma}\",\n",
        "    \"customer_issue\": \"\", \"customer_goal\": \"\", \"agent_goal\": \"\",\n",
        "    \"resolved\": true, \"num_interactions\": 0, \"duration_sec\": 0\n",
        "  }},\n",
        "  \"transcript\": [\n",
        "    {{\"turn\": 1, \"speaker\": \"cliente\", \"text\": \"\", \"timestamp\": \"\"}},\n",
        "    {{\"turn\": 2, \"speaker\": \"agente\",  \"text\": \"\", \"timestamp\": \"\"}}\n",
        "  ],\n",
        "  \"outcomes\": {{\n",
        "    \"csat_estimated_1_5\": 3, \"next_action\": \"\", \"followup_needed\": false, \"summary\": \"\"\n",
        "  }}\n",
        "}}\n",
        "\"\"\".strip()\n",
        "\n",
        "class PromptProvider:\n",
        "    \"\"\"\n",
        "    Permite inyectar parches del LLM.\n",
        "    Si el LLM devuelve textos en prompt_changes.system_patch / user_patch,\n",
        "    los usamos; si no, usamos defaults.\n",
        "    \"\"\"\n",
        "    def __init__(self, system_patch: str | None = None, user_patch: str | None = None):\n",
        "        self.system_patch = (system_patch or \"\").strip() or None\n",
        "        self.user_patch = (user_patch or \"\").strip() or None\n",
        "\n",
        "    def system(self) -> str:\n",
        "        return self.system_patch or default_system_prompt()\n",
        "\n",
        "    def user(self, contexto: str, tono: str, idioma: str, canal: str) -> str:\n",
        "        if self.user_patch:\n",
        "            # Si el patch incluye placeholders, los resolvemos\n",
        "            return self.user_patch.format(contexto=contexto, tono=tono, idioma=(\"español\" if idioma==\"es\" else \"inglés\"), canal=canal)\n",
        "        return default_user_prompt(contexto, tono, idioma, canal)\n",
        "\n",
        "# ========= funciones de generación (compatibles con tu implementación) =========\n",
        "\n",
        "def ensure_defaults(data: Dict[str, Any], contexto: str, canal: str, tono: str, idioma: str) -> Dict[str, Any]:\n",
        "    data.setdefault(\"meta\", {}); data.setdefault(\"transcript\", []); data.setdefault(\"outcomes\", {})\n",
        "    meta = data[\"meta\"]; tx = data[\"transcript\"]; outc = data[\"outcomes\"]\n",
        "    meta.setdefault(\"conversation_id\", str(uuid.uuid4()))\n",
        "    meta[\"company\"] = \"Kavak\"; meta[\"context\"] = contexto\n",
        "    meta[\"channel\"] = meta.get(\"channel\") or canal\n",
        "    meta[\"tone\"] = meta.get(\"tone\") or tono\n",
        "    meta[\"language\"] = meta.get(\"language\") or idioma\n",
        "    meta.setdefault(\"customer_issue\",\"\"); meta.setdefault(\"customer_goal\",\"\"); meta.setdefault(\"agent_goal\",\"\")\n",
        "    meta.setdefault(\"resolved\", True); meta.setdefault(\"num_interactions\", 0); meta.setdefault(\"duration_sec\", 0)\n",
        "\n",
        "    # timestamps\n",
        "    t0 = datetime.utcnow()\n",
        "    if not tx:\n",
        "        tx.extend([\n",
        "            {\"turn\":1,\"speaker\":\"cliente\",\"text\":\"Hola, ¿me pueden apoyar?\",\"timestamp\":\"\"},\n",
        "            {\"turn\":2,\"speaker\":\"agente\",\"text\":\"Con gusto, ¿puedes compartirme el folio o placas?\",\"timestamp\":\"\"}\n",
        "        ])\n",
        "    for i, t in enumerate(tx):\n",
        "        t[\"turn\"] = i+1\n",
        "        if t.get(\"speaker\") not in (\"cliente\",\"agente\"):\n",
        "            t[\"speaker\"] = \"cliente\" if i % 2 == 0 else \"agente\"\n",
        "        t[\"text\"] = t.get(\"text\") or \"\"\n",
        "        t[\"timestamp\"] = t.get(\"timestamp\") or (t0 + timedelta(seconds=5*i)).isoformat() + \"Z\"\n",
        "\n",
        "    meta[\"num_interactions\"] = len(tx)\n",
        "    estimated = max((len(tx)-1)*5, 45)\n",
        "    try: given = int(meta.get(\"duration_sec\") or 0)\n",
        "    except: given = 0\n",
        "    meta[\"duration_sec\"] = max(given, estimated)\n",
        "\n",
        "    outc.setdefault(\"csat_estimated_1_5\", 3)\n",
        "    outc.setdefault(\"next_action\",\"\")\n",
        "    outc.setdefault(\"followup_needed\", not bool(meta.get(\"resolved\", True)))\n",
        "    outc.setdefault(\"summary\",\"\")\n",
        "    return data\n",
        "\n",
        "def generate_one_conversation(contexto: str, prompt_provider: PromptProvider, seed: int | None = None) -> Dict[str, Any]:\n",
        "    random.seed(seed or random.randint(1, 10_000))\n",
        "    tono = random.choice(TONOS); canal = random.choice(CANALES); idioma = random.choice(IDIOMAS)\n",
        "    uprompt = prompt_provider.user(contexto, tono, idioma, canal)\n",
        "    resp = client.chat.completions.create(\n",
        "        model=MODEL,\n",
        "        temperature=1.0,\n",
        "        response_format={\"type\":\"json_object\"},\n",
        "        messages=[\n",
        "            {\"role\":\"system\",\"content\": prompt_provider.system()},\n",
        "            {\"role\":\"user\",\"content\": uprompt}\n",
        "        ]\n",
        "    )\n",
        "    content = (resp.choices[0].message.content or \"\").strip()\n",
        "    if content.startswith(\"```\"):\n",
        "        content = content.strip(\"`\")\n",
        "        lines = content.splitlines()\n",
        "        if lines and lines[0].lower().startswith(\"json\"):\n",
        "            content = \"\\n\".join(lines[1:])\n",
        "    try:\n",
        "        data = json.loads(content)\n",
        "    except json.JSONDecodeError:\n",
        "        data = {\"meta\": {}, \"transcript\": [], \"outcomes\": {}}\n",
        "    return ensure_defaults(data, contexto, canal, tono, idioma)\n",
        "\n",
        "def _gen_with_retries(contexto: str, prompt_provider: PromptProvider, seed: int, max_attempts=4, base_delay=1.3):\n",
        "    attempt = 0\n",
        "    while True:\n",
        "        try:\n",
        "            return generate_one_conversation(contexto, prompt_provider, seed=seed)\n",
        "        except Exception as e:\n",
        "            attempt += 1\n",
        "            if attempt >= max_attempts:\n",
        "                raise\n",
        "            sleep_s = (base_delay ** attempt) + (0.25 * random.random())\n",
        "            print(f\"⚠️  Error {contexto} (seed={seed}). Retry {attempt}/{max_attempts-1} in {sleep_s:.2f}s -> {e}\")\n",
        "            time.sleep(sleep_s)\n",
        "\n",
        "def generate_dataset_parallel(contexts: List[str], prompt_provider: PromptProvider, per_context=2, seed=123, max_workers=8, max_attempts=4) -> List[Dict[str, Any]]:\n",
        "    random.seed(seed)\n",
        "    tasks = [(c, random.randint(1,10_000)) for c in contexts for _ in range(per_context)]\n",
        "    out: List[Dict[str,Any]] = []\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as ex:\n",
        "        futs = {ex.submit(_gen_with_retries, c, prompt_provider, s, max_attempts): (c,s) for (c,s) in tasks}\n",
        "        iterator = tqdm(as_completed(futs), total=len(tasks), desc=\"Generando propuestas\", unit=\"conv\")\n",
        "        for fut in iterator:\n",
        "            c,s = futs[fut]\n",
        "            try:\n",
        "                out.append(fut.result())\n",
        "            except Exception as e:\n",
        "                print(f\"❌ Falló definitivamente {c} (seed={s}): {e}\")\n",
        "    return out\n",
        "\n",
        "# ===================== 3) APLICAR PATCHES Y COMPARAR =====================\n",
        "\n",
        "def compare_and_report(baseline_convs: List[Dict[str, Any]], proposed_convs: List[Dict[str, Any]], out_dir: Path) -> str:\n",
        "    mb = metrics_from_convs(baseline_convs)\n",
        "    mp = metrics_from_convs(proposed_convs)\n",
        "\n",
        "    # export CSVs por si quieres inspeccionar\n",
        "    save_jsonl(proposed_convs, out_dir.joinpath(\"proposed_conversations.jsonl\").as_posix())\n",
        "\n",
        "    # informe simple\n",
        "    md = []\n",
        "    md.append(\"# Comparativo Baseline vs Proposed\\n\")\n",
        "    md.append(\"## Baseline\\n\")\n",
        "    md.append(f\"- total: {mb['total']}\\n- resolution_rate: {mb['resolution_rate']}\\n- avg_csat: {mb['avg_csat']}\\n- avg_turns: {mb['avg_turns']}\\n- avg_duration_sec: {mb['avg_duration_sec']}\\n\")\n",
        "    md.append(\"## Proposed\\n\")\n",
        "    md.append(f\"- total: {mp['total']}\\n- resolution_rate: {mp['resolution_rate']}\\n- avg_csat: {mp['avg_csat']}\\n- avg_turns: {mp['avg_turns']}\\n- avg_duration_sec: {mp['avg_duration_sec']}\\n\")\n",
        "\n",
        "    # deltas\n",
        "    def dstr(a, b):\n",
        "        if a is None or b is None: return \"n/a\"\n",
        "        d = b - a\n",
        "        sign = \"▲\" if d > 0 else (\"▼\" if d < 0 else \"＝\")\n",
        "        return f\"{b} ({sign} {d:+.3f})\" if isinstance(b, float) else f\"{b} ({sign} {d})\"\n",
        "\n",
        "    md.append(\"## Deltas (Proposed - Baseline)\\n\")\n",
        "    md.append(f\"- resolution_rate: {dstr(mb['resolution_rate'], mp['resolution_rate'])}\")\n",
        "    md.append(f\"- avg_csat: {dstr(mb['avg_csat'], mp['avg_csat'])}\")\n",
        "    md.append(f\"- avg_turns: {dstr(mb['avg_turns'], mp['avg_turns'])}\")\n",
        "    md.append(f\"- avg_duration_sec: {dstr(mb['avg_duration_sec'], mp['avg_duration_sec'])}\\n\")\n",
        "\n",
        "    report_path = out_dir.joinpath(\"comparison_report.md\")\n",
        "    with open(report_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\\n\".join(md))\n",
        "    return report_path.as_posix()\n",
        "\n",
        "# ===================== MAIN =====================\n",
        "\n",
        "def main():\n",
        "    # 0) Cargar baseline (si no existe, avisa)\n",
        "    if not os.path.exists(BASELINE_INPUT_JSONL):\n",
        "        raise FileNotFoundError(f\"No existe baseline JSONL: {BASELINE_INPUT_JSONL}\")\n",
        "    baseline_convs = read_jsonl(BASELINE_INPUT_JSONL)\n",
        "\n",
        "    # 1) LLM propone cambios\n",
        "    llm_out = run_llm_proposal(baseline_convs)\n",
        "    with open(OUT_DIR.joinpath(\"llm_rewrite_response.json\"), \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(llm_out, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    # 2) Construir PromptProvider con parches\n",
        "    pc = (llm_out.get(\"prompt_changes\") or {})\n",
        "    system_patch = pc.get(\"system_patch\") or \"\"\n",
        "    user_patch = pc.get(\"user_patch\") or \"\"\n",
        "    prompts = PromptProvider(system_patch=system_patch, user_patch=user_patch)\n",
        "\n",
        "    # 3) Re-generar dataset propuesto en paralelo\n",
        "    proposed = generate_dataset_parallel(\n",
        "        CONTEXTS, prompts, per_context=PER_CONTEXT, seed=123,\n",
        "        max_workers=MAX_WORKERS, max_attempts=MAX_ATTEMPTS\n",
        "    )\n",
        "\n",
        "    # 4) Comparar y reportar\n",
        "    comp_path = compare_and_report(baseline_convs, proposed, OUT_DIR)\n",
        "\n",
        "    # 5) Guardar meta CSVs opcionales\n",
        "    def meta_rows(convs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        out = []\n",
        "        for r in convs:\n",
        "            meta = r.get(\"meta\", {}); outc = r.get(\"outcomes\", {})\n",
        "            out.append({\n",
        "                \"conversation_id\": meta.get(\"conversation_id\",\"\"),\n",
        "                \"context\": meta.get(\"context\",\"\"),\n",
        "                \"resolved\": meta.get(\"resolved\", True),\n",
        "                \"num_interactions\": meta.get(\"num_interactions\", 0),\n",
        "                \"duration_sec\": meta.get(\"duration_sec\", 0),\n",
        "                \"csat_estimated_1_5\": outc.get(\"csat_estimated_1_5\"),\n",
        "                \"summary\": outc.get(\"summary\",\"\"),\n",
        "            })\n",
        "        return out\n",
        "\n",
        "    baseline_meta_csv = OUT_DIR.joinpath(\"baseline_meta.csv\")\n",
        "    proposed_meta_csv = OUT_DIR.joinpath(\"proposed_meta.csv\")\n",
        "    pd.DataFrame(meta_rows(baseline_convs)).to_csv(baseline_meta_csv, index=False)\n",
        "    pd.DataFrame(meta_rows(proposed)).to_csv(proposed_meta_csv, index=False)\n",
        "\n",
        "    print(\"✅ Listo.\")\n",
        "    print(\"LLM JSON:\", OUT_DIR.joinpath(\"llm_rewrite_response.json\").as_posix())\n",
        "    print(\"Reporte:\", comp_path)\n",
        "    print(\"Baseline meta:\", baseline_meta_csv.as_posix())\n",
        "    print(\"Proposed meta:\", proposed_meta_csv.as_posix())\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a9dadf8d1d8f49e4a28758346e36266a",
            "fa3e6ec3e26a472aa8f786c592f4f54f",
            "993c2a1eea4b42689fd139c7b6c3d143",
            "7d8873da0123498dab0c873627edb86d",
            "30f2837c52af4ab38817be3a8e908d96",
            "378b193c5b774abba2017c47d67ff944",
            "b5bb9fb4a9a94dbb822b3beba3e791ef",
            "ea47bd182010402ab605a60b7f3d6187",
            "ccceb035d88d4dad93b856a75ea8e003",
            "9a8ae9cdace84639a77bbae64ad28bee",
            "8bbbfcd52abe4ba09a22b8f68ec32973"
          ]
        },
        "id": "iBYHQ9KVJzP4",
        "outputId": "356aebc1-fc0e-4952-a97f-4c8304fdc71f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generando propuestas:   0%|          | 0/18 [00:00<?, ?conv/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9dadf8d1d8f49e4a28758346e36266a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️  Error buying (seed=858). Retry 1/3 in 1.45s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=1429). Retry 1/3 in 1.33s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=626). Retry 1/3 in 1.34s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=4368). Retry 1/3 in 1.36s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=1765). Retry 1/3 in 1.46s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=6212). Retry 1/3 in 1.37s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=6673). Retry 1/3 in 1.38s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=4386). Retry 1/3 in 1.38s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=1429). Retry 2/3 in 1.84s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=626). Retry 2/3 in 1.72s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=4368). Retry 2/3 in 1.81s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=858). Retry 2/3 in 1.78s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=6212). Retry 2/3 in 1.77s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=6673). Retry 2/3 in 1.85s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=1765). Retry 2/3 in 1.83s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=4386). Retry 2/3 in 1.69s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=626). Retry 3/3 in 2.21s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=1429). Retry 3/3 in 2.24s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=4368). Retry 3/3 in 2.41s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=858). Retry 3/3 in 2.35s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=6212). Retry 3/3 in 2.28s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=6673). Retry 3/3 in 2.36s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error buying (seed=4386). Retry 3/3 in 2.32s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error ask (seed=1765). Retry 3/3 in 2.29s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente feedback (seed=626): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente buying (seed=1429): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=9214). Retry 1/3 in 1.32s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=8786). Retry 1/3 in 1.45s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente ask (seed=4368): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente buying (seed=858): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5443). Retry 1/3 in 1.38s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5584). Retry 1/3 in 1.46s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente feedback (seed=6212): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente ask (seed=6673): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2616). Retry 1/3 in 1.44s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=851). Retry 1/3 in 1.30s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente buying (seed=4386): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2213). Retry 1/3 in 1.42s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente ask (seed=1765): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5525). Retry 1/3 in 1.35s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=9214). Retry 2/3 in 1.90s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=8786). Retry 2/3 in 1.87s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5443). Retry 2/3 in 1.80s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=851). Retry 2/3 in 1.81s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5584). Retry 2/3 in 1.93s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2616). Retry 2/3 in 1.84s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2213). Retry 2/3 in 1.87s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5525). Retry 2/3 in 1.74s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=9214). Retry 3/3 in 2.40s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5443). Retry 3/3 in 2.45s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error feedback (seed=8786). Retry 3/3 in 2.31s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=851). Retry 3/3 in 2.30s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error service (seed=5584). Retry 3/3 in 2.31s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2616). Retry 3/3 in 2.44s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error credit (seed=2213). Retry 3/3 in 2.25s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5525). Retry 3/3 in 2.36s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente service (seed=9214): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente feedback (seed=8786): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=9191). Retry 1/3 in 1.48s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente credit (seed=851): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente service (seed=5443): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente service (seed=5584): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente credit (seed=2616): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente credit (seed=2213): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente warranty (seed=5525): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=9191). Retry 2/3 in 1.92s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5469). Retry 1/3 in 1.45s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5469). Retry 2/3 in 1.87s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=9191). Retry 3/3 in 2.43s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "⚠️  Error warranty (seed=5469). Retry 3/3 in 2.38s -> Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente warranty (seed=9191): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "❌ Falló definitivamente warranty (seed=5469): Error code: 400 - {'error': {'message': \"'messages' must contain the word 'json' in some form, to use 'response_format' of type 'json_object'.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': None}}\n",
            "✅ Listo.\n",
            "LLM JSON: rewriter_apply_out/llm_rewrite_response.json\n",
            "Reporte: rewriter_apply_out/comparison_report.md\n",
            "Baseline meta: rewriter_apply_out/baseline_meta.csv\n",
            "Proposed meta: rewriter_apply_out/proposed_meta.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "file_path = '/content/rewriter_apply_out/comparison_report.md'\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    markdown_content = f.read()\n",
        "\n",
        "display(Markdown(markdown_content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "MCeCSll6JzLk",
        "outputId": "cd9d704e-9aa7-4e70-cdd9-8e62f5c4fad1"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Comparativo Baseline vs Proposed\n\n## Baseline\n\n- total: 102\n- resolution_rate: 0.304\n- avg_csat: 4.284\n- avg_turns: 5.26\n- avg_duration_sec: 691.4\n\n## Proposed\n\n- total: 0\n- resolution_rate: 0\n- avg_csat: None\n- avg_turns: 0\n- avg_duration_sec: 0\n\n## Deltas (Proposed - Baseline)\n\n- resolution_rate: 0 (▼ -0.304)\n- avg_csat: n/a\n- avg_turns: 0 (▼ -5.26)\n- avg_duration_sec: 0 (▼ -691.4)\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xtp7cF3WJRho"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f5c61cf"
      },
      "source": [
        "{{markdown_content}}"
      ]
    }
  ]
}